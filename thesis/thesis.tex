\documentclass[12pt]{article}
\usepackage{bbm,fullpage,amsthm}
\usepackage{alltt}
\usepackage{bm}
\usepackage{amsmath,amssymb,epsfig,color,xspace,mathrsfs}
\usepackage{bbm,fullpage}
\usepackage{amsmath}
\usepackage{alltt, amssymb, amsthm}
\usepackage{algorithm, pseudocode}
\usepackage{mathrsfs}
\usepackage[colorlinks=true,linkcolor=cyan]{hyperref}
\usepackage{bm}
\pagestyle{empty}

\usepackage{enumerate}
\usepackage{fullpage}
\usepackage[colorlinks=true,linkcolor=cyan]{hyperref}
\usepackage{color}
\newcommand{\todo}[1]{\textcolor{red}{#1}}
\newcommand{\fixme}[1]{\textcolor{blue}{#1}}

% for enumerate / itemize: define reasonable margins
\usepackage[shortlabels]{enumitem}
\setlist{topsep=0.25\baselineskip,partopsep=0pt,itemsep=1pt,parsep=0pt}

% math and theorem names
\usepackage{amsmath,amsfonts,amssymb,amsthm,thmtools}
\declaretheorem[style=plain,parent=section]{definition}
\declaretheorem[sibling=definition]{theorem}
\declaretheorem[sibling=definition]{corollary}
\declaretheorem[sibling=definition]{proposition}
\declaretheorem[sibling=definition]{lemma}
\declaretheorem[style=remark,sibling=definition,qed={\qedsymbol}]{remark}
\declaretheorem[style=remark,sibling=definition,qed={\qedsymbol}]{example}
\declaretheoremstyle[headpunct={},notebraces={\textbf{--}~}{}]{algorithm}
\declaretheorem[style=algorithm]{problem}

% for our algorithms & problems
\usepackage{mdframed}

\usepackage[capitalize]{cleveref}
\crefname{problem}{Problem}{Problems}
\Crefname{problem}{Problem}{Problems}

%%%notation
%misc
\newcommand{\storeArg}{} % aux, not to be used in document!!
\newcounter{notationCounter}
%spaces
\newcommand{\NN}{\mathbb{N}} % nonnegative integers
\newcommand{\var}{T} % variable for univariate polynomials
\newcommand{\field}{\mathbb{K}} % base field
\newcommand{\polRing}{\field[\var]} % polynomial ring
\newcommand{\Pox}{[\mkern-3mu[ \var ]\mkern-3.2mu]}
\newcommand{\Poxi}{[\mkern-3mu[ \var^{-1} ]\mkern-3.2mu]}
\newcommand{\psRing}{\field\Pox}
\newcommand{\matSpace}[1][\rdim]{\renewcommand\storeArg{#1}\matSpaceAux} % polynomial matrix space, 2 opt args
\newcommand{\matSpaceAux}[1][\storeArg]{\field^{\storeArg \times #1}} % not to be used in document
\newcommand{\polMatSpace}[1][\rdim]{\renewcommand\storeArg{#1}\polMatSpaceAux} % polynomial matrix space, 2 opt args
\newcommand{\polMatSpaceAux}[1][\storeArg]{\polRing^{\storeArg \times #1}} % not to be used in document
\newcommand{\psMatSpace}[1][\rdim]{\renewcommand\storeArg{#1}\psMatSpaceAux} % polynomial matrix space, 2 opt args
\newcommand{\psMatSpaceAux}[1][\storeArg]{\psRing^{\storeArg \times #1}} % not to be used in document
\newcommand{\mat}[1]{\mathbf{\MakeUppercase{#1}}} % for a matrix
\newcommand{\row}[1]{\mathbf{\MakeLowercase{#1}}} % for a matrix
\newcommand{\col}[1]{\mathbf{\MakeLowercase{#1}}} % for a matrix
\newcommand{\matCoeff}[1]{\MakeLowercase{#1}} % for a coefficient in a matrix
\newcommand{\rdim}{m} % row dimension
\newcommand{\cdim}{n'} % column dimension
\newcommand{\diag}[1]{\mathrm{Diag}(#1)}  % diagonal matrix with diagonal entries #1
\newcommand{\seqelt}[1]{S_{#1}} % element of sequence of matrices
\newcommand{\seqeltSpace}{\matSpace[\rdim][\cdim]} % element of sequence of matrices
\newcommand{\seq}{\mathcal{S}} % sequence of matrices
\newcommand{\seqpm}{\mat{Z}} % power series matrix from a sequence
\newcommand{\rel}{\col{p}} % linear relation
\newcommand{\relbas}{\mat{P}} % linear relation
\newcommand{\relSpace}{\polMatSpace[1][\rdim]} % space for linear relations
\newcommand{\relbasSpace}{\polMatSpace[\rdim][\rdim]} % space for linear relations
\newcommand{\num}{\row{q}} % numerator for linear recurrence relation
\newcommand{\nummat}{\mat{Q}} % numerator for linear recurrence relation basis
\newcommand{\rem}{\row{r}} % remnant for linear recurrence relation
\newcommand{\remmat}{\mat{R}} % remnant for linear recurrence relation basis
\newcommand{\remSpace}{\polMatSpace[1][\cdim]} % space for linear relations
\newcommand{\degBd}{d} % bound on degree of minimal generator
\newcommand{\degBdr}{d_{r}} % bound on degree of a right minimal generator
\newcommand{\degBdl}{d_{\ell}} % bound on degree of a left minimal generator
\newcommand{\degDet}[1][\seq]{\operatorname{\Delta}(#1)}
\newcommand{\rdeg}[2][]{\mathrm{rdeg}_{{#1}}(#2)} % shifted row degree
\newcommand{\cdeg}[2][]{\mathrm{cdeg}_{{#1}}(#2)} % shifted column degree
\newcommand{\sys}{\mat{F}} % input matrix series to approximant basis
\newcommand{\appMod}[2]{\mathcal{A}(#1,#2)} % module of approximants for #2 at order #1

%% ------------------------------------------------
%% --------- For problems and algorithms ----------
%% ------------------------------------------------
\newcommand{\argfig}[1]{\begin{figure}[#1]} % to be able to feed an optional argument to the inside figure
	
	\newenvironment{algobox}[1][htbp]{
		\newcommand{\algoInfo}[3]{
			\begin{algorithm}[{name=[\algoname{##2}:~##1]\algoname{##2}}]
				\label{##3}
				~ \hfill
				{\small\emph{(##1)}}
				\smallskip
				
			}
			\newcommand{\dataInfos}[2]{
				\algoword{##1:}
				\begin{itemize}[topsep=0pt]
					##2
				\end{itemize}
				\smallskip
			}
			\newcommand{\dataInfo}[2]{
				\algoword{##1:} ##2 
				%\smallskip
			}
			\newcommand{\algoSteps}[1]{
				%\smallskip
				\addtolength{\leftmargini}{-0.35cm}
				\begin{enumerate}[{\bf 1.}]
					##1
				\end{enumerate}
				\smallskip
			}
			
			\expandafter\argfig\expandafter{#1}
			\centering
			\begin{minipage}{0.99\textwidth}
				\begin{mdframed}
					\smallskip
				}
				{
				\end{algorithm}
			\end{mdframed}
		\end{minipage}
	\end{figure}
}

\newenvironment{problembox}[1][htbp]{
	\newcommand{\problemInfo}[3]{
		\begin{problem}[{name=[\emph{##2}\ifx&##1&\else##1\fi]\emph{##2}}]
			\label{##3}
			~\smallskip
			
		}
		\newcommand{\dataInfos}[2]{
			\emph{##1:}
			\begin{itemize}[topsep=0pt]
				##2
			\end{itemize}
			\smallskip
		}
		\newcommand{\dataInfo}[2]{
			\emph{##1:}  ##2
		}
		
		\expandafter\argfig\expandafter{#1}
		\centering
		\begin{minipage}{0.75\textwidth}
			\begin{mdframed}
			}
			{
			\end{problem}
		\end{mdframed}
	\end{minipage}
\end{figure}
}


\newtheorem*{Example}{Example}
\newtheorem{Coro}{Corollary}
\newtheorem{Def}{Definition}
\newtheorem{Theo}{Theorem}
\newtheorem{Prop}{Proposition}
\newtheorem{Lemma}{Lemma}

\def\C {\ensuremath{\mathbb{C}}}
\def\Q {\ensuremath{\mathbb{Q}}}
\def\N {\ensuremath{\mathbb{N}}}
\def\R {\ensuremath{\mathbb{R}}}
\def\Z {\ensuremath{\mathbb{Z}}}
\def\F {\ensuremath{\mathbb{F}}}
\def\H {\ensuremath{\mathbb{H}}}
\def\K {\ensuremath{\mathbb{K}}}
\def\Kbar {{\ensuremath{\overline{\mathbb{K}}}}}
\def\A {\ensuremath{\mathbb{A}}}
\def\D {\ensuremath{D}}
\def\m {\ensuremath{\mathfrak{m}}}
\def\todo#1{(\textbf{todo:} #1)}

\def\scrM {\ensuremath{\mathscr{M}}}
\def\calL {\ensuremath{\mathcal{L}}}
\def\scrP {\ensuremath{\mathscr{P}}}
\def\scrS {\ensuremath{\mathscr{S}}}
\def\scrU {\ensuremath{\mathscr{U}}}
\def\scrV {\ensuremath{\mathscr{V}}}
\def\ann {\ensuremath{\mathrm{ann}}}
\def\rk {\ensuremath{\mathrm{rk}}}

\DeclareBoldMathCommand{\bell}{\ell}
\DeclareBoldMathCommand{\bu}{u}
\DeclareBoldMathCommand{\bv}{v}
\DeclareBoldMathCommand{\bX}{X}
\DeclareBoldMathCommand{\bx}{x}
\DeclareBoldMathCommand{\balpha}{\alpha}
\DeclareBoldMathCommand{\bbeta}{\beta}

\def\K{\mathbb{K}}
\def\Deg{D}
\def\scrY{\mathscr{Y}}
\def\mA{\mathbf{A}}
\def\mB{\mathbf{B}}
\def\mD{\mathbf{D}}
\def\mF{\mathbf{F}}
\def\mG{\mathbf{G}}
\def\mI{\mathbf{I}}
\def\mM{\mathbf{M}}
\def\mNs{\mathbf{N^*}}
\def\mN{\mathbf{N}}
\def\mS{\mathbf{S}}
\def\mT{\mathbf{T}}
\def\mU{\mathbf{U}}
\def\mV{\mathbf{V}}
\def\mW{\mathbf{W}}
\def\mX{\mathbf{X}}
\def\mY{\mathbf{Y}}
\def\mZ{\mathbf{Z}}

\title{Block Sparse-FGLM Algorithm}

\thispagestyle{empty}

\begin{document}

\tableofcontents
\pagebreak

\section{Introduction}
Computing the Gr\"obner basis of an ideal with respect to a term
ordering is an essential step in solving systems of polynomials. 
Certain term orderings, such as the degree reverse lexicographical 
ordering (\textit{degrevlex}), the computation of the Gr\"obner basis
faster, while other orderings, such as the lexicographical ordering
(\textit{lex}), make it easier to find the coordinates of the solutions.
In particular, for a radical ideal with a finite set of points in its variety
and in generic position, its lex Gr\"obner basis has the form
$$ \{  x_1 - R_1(x_n), x_2-R_2(x_n),\dots, x_{n-1}-R_{n-1}(x_n), R_n(x_n)  \}$$
where $R_i$ is a polynomial in only $x_n$. The points in the variety are
$$\{ ( R_1(\alpha), R_2(\alpha), \dots, R_n(\alpha)  ) \mid \forall \alpha: R_n(\alpha)=0  \}$$
Thus, one typically first computes a Gr\"obner basis for the degrevlex ordering,
and then converts it to a lex Gr\"obner basis, or a related representation, such as 
Rouillier's Rational Univariate Representation \cite{Rouillier99}. One such algorithm is
the Sparse-FGLM, which can be seen as an application
of the Wiedemann algorithm. In this document, we will
introduce a new algorithm inspired by Sparse-FGLM
and block Wiedemann algorithm that is easily
parallelizable.

\subsection{Basic Operations}
Over a field $\mathbb{K}$,
recall that we can compute multiplication,
division with remainder, extended GCD, and square free part
of polynomial of degree at most $n$ in $O^{\tilde{~}}(n)$
field operations ($O^{\tilde{~}}$ omits polylogarithmic factors)
\cite{GaGe13}. 


\newpage
\section{Previous Algorithms}

\subsection{Wiedemann algorithm}
The Wiedemann algorithm of \cite{Wiedemann86} solves a linear system
$\mA y = b$, where $\mA$ is an invertible square matrix and $y,b$ are vectors
over a field. The most
important aspect of this algorithm is that the minimal polynomial
$P(x) = \sum_{i = 0}^{D} c_i x^i$ of $\mA$ can be computed
through a scalar sequence
$$ S = (u^{tr}\mA^ib)_{i \ge 0}$$
for a random vector $u$. Once we have sufficient number of terms
of $S$, we apply the Berlekamp-Massey algorithm, which efficiently
computes $P(x)$ from $S$.
In the simplest case, where $c_0 \neq 0$, we have that
\begin{align*}
P(\mA) = 0 &= c_0 \mI + c_1\mA + \dots + c_D \mA^D \\
\implies -c_0 \mI &= \sum_{i=1}^{D}c_i\mA^i \\
\implies \mI &= \mA\left(-c_0^{-1} \sum_{i=1}^{D}c_i\mA^{i-1}\right)\\
\implies b &= \mA(-c_0^{-1} \sum_{i=1}^{D}c_i\mA^{i-1})b
\end{align*}
Therefore, $x = \left(-c_0^{-1} \sum_{i=1}^{D}c_i\mA^{i-1}\right)b$.

\subsection{Sparse-FGLM Algorithm}
The Sparse-FGLM algorithm \cite{FaMo17} computes the lex
Gr\"obner basis of an ideal with runtime cubic in the dimension
of the monomial basis. More precisely, given

\begin{center}
\begin{tabular}{c c}
	$I \subset \mathbb{K}$:& zero dimensional radical ideal
	in shape position\\
	$\mathbb{B} \subset \mathbb{K}[X_1,\dots,X_n]/I$:&
	monomial basis of $\mathbb{K}[X_1,\dots,X_n]/I$\\
	$D$: & dimension of $\mathbb{B}$\\
	$\mM_1,\dots,\mM_n$:& multiplication matrices of
	$X_1,\dots,X_n$ respectively	
\end{tabular}
\end{center}
it produces the lex Gr\"obner basis of $I$ of the form
$\{ P(X_1), X_2-R_2(X_1),\dots, X_n - R_n(X_1)  \}$.
The key idea is that $P(X_1)$ is the minimal polynomial
of the multiplication matrix $\mM_1$, which we can
find using the Wiedemann algorithm. We generate
$$ S = (u^{tr} \mM^i_1 e)_{(0 \le i < 2D)}$$
where $u$ is a random vector and $e$ is the coordinate
vector for $1$ in $\mathbb{B}$. Then, we find the minimum
generating polynomial $P$ by
applying the Berlekamp-Massey algorithm on $S$.
We compute the numerator $n$ of the generating series
$Z = \sum_{i=0}^{D-1} u^{tr} \mM^i_1 e/T^{i+1}$ by a product
$N = P Z$. To find $R_j(x_1)$, $2 \le j \le n$, we first
compute the numerator $N_j$ of the generating series
$Z_j = \sum_{i=0}^{D} u^{tr} \mM_1^i \mM_j e / T^{i+1}$ by a product
$N_j = P Z_j$. Finally, $R_j(x_1) = \frac{N_j}{N} \mod P$.
More general versions of the Sparse-FGLM algorithm exist to handle
non-radical ideals using the Berlekamp-Massey-Sakata algorithm \cite{FaMo17}.

\newpage
\section{Previously Results on Generating Series}

\subsection{Overview}
In what follows, $\K$ is a field.  Let $I$ be an ideal in
$\K[X_1,\dots,X_n]$ and $Q=\K[X_1,\dots,X_n]/I$ be the associated
residue class ring. Suppose that $V=V(I)$ has dimension zero, and
write it as $V=\{\balpha_1,\dots,\balpha_d\},$ with all $\balpha_i$'s
in $\Kbar^n$, and $\balpha_i=(\alpha_{i,1},\dots,\alpha_{i,n})$ for
all $i$.  We also let $\D$ be the dimension of $Q$, so that $d \le
\D$, and {\em we assume that ${\rm char}(\K)$ is greater than $D$}. In
this section, we recall and generalize results from the appendix
of~\cite{BoSaSc03}, with the objective of computing a zero-dimensional
parametrization of $V$.

The main novelty in our approach is to avoid generic coordinates as
much as possible. The algorithm will decompose $V$ into two parts: for
the first part, we will be able to use $X_1$ as a separating element;
the remaining points will be dealt with using a random linear
form. Throughout, we rely only the following operations: evaluations
of linear forms on successive powers of a given element in $Q$, say
$1,t,t^2,\dots$, and elementary operations on univariate polynomials.

The main algorithm is as follows. For the moment, we can only describe
its main structure; the details of the subroutines are given in the
next paragraphs.

\begin{algorithm}[H]
	\caption{$\mathsf{Parametrization}(\ell,t)$} {\bf
		Input:} \vspace{-0.5em}
	\begin{itemize}\setlength\itemsep{0em}
		\item a linear form $\ell$ over $Q$
		\item $t=t_1 X_1 + \cdots + t_n X_n$
	\end{itemize}
	{\bf Output:} \vspace{-0.5em}
	\begin{itemize}\setlength\itemsep{0em}
		\item polynomials $(P,(V_1,\dots,V_n))$
	\end{itemize}
	\begin{enumerate}
		\item let $(F,(G_1,\dots,G_n),X_1)=\mathsf{ParametrizationX}_1(\ell)$
		\item let $\ell'=\mathsf{Update}(\ell,F,t)$
		\item let $(Q,(W_1,\dots,W_n),t)=\mathsf{ParametrizationGeneric}(\ell',t)$
		\item let $(F^*,(G^*_1,\dots,G^*_n),t)=\mathsf{ChangeCoordinate}(F,(G_1,\dots,G_n),t)$
		\item let  $(P,(V_1,\dots,V_n))=\mathsf{Union}(F^*,(G^*_1,\dots,G^*_n),\, Q,(W_1,\dots,W_n))$
		\item \textbf{return} $(P,(V_1,\dots,V_n),t)$
	\end{enumerate}
\end{algorithm}

The call to $\mathsf{ParametrizationX}_1(\ell)$ computes a
zero-dimensional parametrization of a subset $V'$ of $V$ for which
$X_1$ is a separating element, using values of the form
$(\ell(X_1^s))_{s \ge 0}$. We then modify $\ell$ (which in effect
removes from $V$ the points we just found) and apply
$\mathsf{ParametrizationGeneric}(\ell',t)$, to obtain a
zero-dimensional parametrization of $V''=V-V'$ using values of the form
$(\ell'(t^s))_{s \ge 0}$. The last two steps involve changing
coordinates in $(F,(G_1,\dots,G_n)$ (to use $t$ as a separating
variable instead), and performing the union of the two components $V'$
and $V''$.

\subsubsection{Basic facts on linearly recurrent sequences.} Consider a sequence $(\ell_s)_{s \ge 0} \in \K^\N$
and the associated generating series $Z=\sum_{s \ge 0} \ell_s T^s \in
\K[[T]]$. The sequence $(\ell_s)_{s \ge 0}$ is {\rm linearly recurrent} if and
only if its generating series is {\em rational} that is, if there
exist polynomials $A,B$ in $\K[T]$ such that $Z=A/B$; these
polynomials are unique if we assume $\gcd(A,B)=1$ and $B(0)=1$. 

We say that a degree $m$ polynomial $P\in\K[T]$ {\em cancels} a
sequence $(\ell_s)_{s \ge 0}$ if $p_0 \ell_s + \cdots + p_m
\ell_{s+m}=0$ for all $s \ge 0$, where $p_0,\dots,p_m$ are the
coefficients of $P$; this is equivalent to ${\rm rev}(P) Z$ being a
polynomial of degree less than $m$, with $Z=\sum_{s \ge 0} \ell_s T^s$
and ${\rm rev}(P)=T^m P(1/T)$. The {\em minimal polynomial} of
sequence $(\ell_s)_{s \ge 0}$ is the monic polynomial of lowest degree
that cancels this sequence. 

Given the closed form $Z =  A/B$ as above, we define $\tilde B =
T^{\max(\deg(A)+1,\deg(B))}B(1/T)$.  By construction, $\tilde B$ lies
in $\K[T]$, and one can check that it is the minimal polynomial of
sequence $(\ell_s)_{s \ge 0}$. However, it is often easier to work with a 
closed form that has the minimal polynomial as the denominator rather than
its reverse. Let $Z' = \sum_{s\ge0} \ell_s / T^{i+1}$ and $P$ be a polynomial
that cancels $(\ell_s)_{s\ge0}$, then the sequence 
$(\ell_s)_{s\ge0}$ is linearly recurrent if and only if $Z'P$ is a polynomial
of degree less than $deg(P)$. Therefore, if $A' = Z'P$ and $B' = P$, then we
can write the closed form $Z' = A'/B'$.

In both cases, if we are given the series and the denominator, then our 
choice for the numerator is unique. The next definition captures this idea:
\begin{definition}
	\label{def:omega}
	Let $S=(S_k)_{k \ge 0}$ be a sequence and $P$ be a polynomial that cancels
	$S$, then $\Omega(S,P)$ is the numerator for the closed form of $S$ with 
	respect to $P$ if:
	\begin{itemize}
		\item $\Omega(S,P)$ is a polynomial of degree less than $deg(P)$
		\item $\sum_{k\ge0} S_k / T^{k+1} = \Omega(S,P)/P$
	\end{itemize}
\end{definition}
\noindent If the sequence $(\ell)_{s\ge 0}$  is linearly recurrent and
$P$ cancels it, then $\Omega((\ell)_{s\ge0}, P)$ exists.
Let $A$ and $A'$ be defined as above, then
$$A' = \Omega((\ell)_{s\ge0},P) = T^{deg(P) - 1} A (1/T)$$ 
\noindent This makes it possible to avoid computations using a series
over $1/T$. We give an example of this using
the Fibonacci sequence, which is well known to be linearly recurrent. Since 
the Fibonacci sequence $F = (1,1,2,3,5,8,\dots)$ is defined
recursively as $F_{s+2} = F_{s} + F_{s+1}$, $F_{0}=F_{1} = 1$, it is
easy to verify that the minimal polynomial of $F$ is $P = 1-T-T^2$.
Define $Z = \sum_{s\ge 0} F_{s} T^s$ and ${\rm rev}(P) = T^2-T-1$, then
we can write $Z$ in its closed form:
$$Z = \frac{A}{B} = \frac{1}{T^2-T-1}$$
\noindent Now, define $Z' = \sum_{s\ge0} F_{s}/T^{s+1}$, then we have that
\begin{align*}
\Omega(S,P)& = T^{deg(P)-1}A(1/T) = T\\
\implies Z' &= \frac{T}{1-T-T^2}
\end{align*}

The sequences we consider below are of the form $(\ell(t^s))_{s \ge
	0}$, for $\ell$ a $\K$-linear form $Q \to \K$ and $t$ in $Q$. For such
sequences, the following standard result will be useful.
\begin{Lemma}\label{lemma:minpoly}
	Let $t$ be in $Q$ and let $P \in \K[T]$ be its minimal
	polynomial. For a generic choice of $\ell$ in ${\rm hom}_\K(Q,\K)$,
	$P$ is the minimal polynomial of the sequence $(\ell(t^s))_{s \ge
		0}$.
\end{Lemma}





\subsubsection{Structure of the dual.}
For $i$ in $\{1,\dots,d\}$, let $Q_i$ be the local algebra at
$\balpha_i$, that is $Q_i=\Kbar[X_1,\dots,X_n]/I_i$, with $I_i$ the
$\m_{\balpha_i}$-primary component of $I$. By the Chinese Remainder
Theorem, $Q\otimes_\K \Kbar=\Kbar[X_1,\dots,X_n]/I$ is isomorphic to
the direct product $Q_1\times \cdots \times Q_d$.  We let $N_i$ be the
{\em nil-index} of $Q_i$, that is, the maximal integer $N$ such that
$\m_{\alpha_i}^N$ is not contained in $I_i$; for instance, $N_i=0$ if
and only if $Q_i$ is a field, if and only if $\balpha_i$ is a
non-singular root of $I$. We also let
$\D_i=\dim_\Kbar(Q_i)$, so that we have $D_i \ge N_i$ and $\D=\D_1 + \cdots + \D_d$.

Fix $i$ in $1,\dots,d$.  There exists a basis of the dual ${\rm
	hom}_\Kbar(Q_i,\Kbar)$ consisting of linear forms
$(\lambda_{i,j})_{1\le j \le \D_i}$ of the form
$$\lambda_{i,j}: f \mapsto (\Lambda_{i,j}(f))(\balpha_i),$$
where $\Lambda_{i,j}$ is the operator
$$f \mapsto \Lambda_{i,j}(f) = \sum_{\mu=(\mu_1,\dots,\mu_n) \in
	S_{i,j}} c_{i,j,\mu} \frac{ \partial^{\mu_1 + \cdots + \mu_n} f}
{\partial X_1^{\mu_1} \cdots \partial X_n^{\mu_n}},$$ for some finite
subset $S_{i,j}$ of $\N^n$ and non-zero constants $c_{i,j,\mu}$ in
$\Kbar$. 
%% Let $w_{i,j}$ be the maximum of all $|\mu|$ for $\mu$ in
%% $S_{i,j}$, with $|\mu| = \mu_1 +\cdots + \mu_n $.
%% By~\cite[Lemma~3.3]{Mourrain97}, we have that $\max_j w_{i,j} =N_i$
%% for all $i$.
For instance, when $\balpha_i$ is non-singular, we have $D_i=1$, so
there is only one function $\lambda_{i,j}$, namely $\lambda_{i,1}$, we
write it $\lambda_{i,1}(f) = f(\balpha_i)$.

More generally, we can always take $\lambda_{i,1}$ of the form
$\lambda_{i,1}(f) = f(\balpha_i)$; for $j>1$, we can then also assume
that $S_{i,j}$ does not contain $\mu=(0,\dots,0)$ (that is, all terms
in $\Lambda_{i,j}$ have order $1$ or more). Thus, introducing new
variables $(U_{i,j})_{j =1,\dots,D_i}$, we deduce the existence of
non-zero homogeneous linear forms $P_{i,\mu}$ in
$(U_{i,j})_{j=1,\dots,D_i}$ such that for any $\lambda$ in ${\rm
	hom}_\Kbar(Q_i,\Kbar)$, there exist $\bu_i=(u_{i,j}) \in
\Kbar{}^{D_i}$ such that we have
\begin{align}\label{ell_param}
\lambda: f \mapsto \lambda(f)
&= \sum_{j=1}^{D_i} u_{i,j} \lambda_{i,j}(f)\nonumber\\
&= \sum_{j=1}^{D_i} u_{i,j} \big(\Lambda_{i,j}(f)\big)(\balpha_i)\nonumber\\
&= \sum_{j=1}^{D_i} u_{i,j}
\sum_{\mu=(\mu_1,\dots,\mu_n) \in
	S_{i,j}} c_{i,j,\mu} \frac{ \partial^{\mu_1 + \cdots + \mu_n} f}
{\partial X_1^{\mu_1} \cdots \partial X_n^{\mu_n}}(\balpha_i)\nonumber\\
&= \sum_{\mu=(\mu_1,\dots,\mu_n) \in S_i} P_{i,\mu}(\bu_i)
\frac{ \partial^{\mu_1 + \cdots + \mu_n} f}
{\partial X_1^{\mu_1} \cdots \partial X_n^{\mu_n}}(\balpha_i),
\end{align}
where $S_i$ is  the union of $S_{i,1},\dots,S_{i,D_i}$,
with in particular $P_{i,(0,\dots,0)}=u_{i,1}$ and where $P_{i,\mu}$
depends only on $(u_{i,j})_{j =2,\dots,D_i}$ for all $\mu$ in $S_i$,
$\mu \ne (0,\dots,0)$. Explicily, we can write $P_{i,\mu}=\sum_{j\in
	\{1,\dots,D_i\} \text{~such that~} \mu \in S_{i,j}} c_{i,j,\mu}
U_{i,j}$.

Fix $\lambda$ non-zero in ${\rm hom}_\Kbar(Q_i,\Kbar)$. We can then
define its {\em order} $w$ and {\em symbol} $\pi$. The former is
the maximum of all $|\mu|=\mu_1+\cdots+\mu_n$ for
$\mu=(\mu_1,\dots,\mu_n)$ in $S_i$ such that $P_{i,\mu}(\bu_i)$ is
non-zero; by~\cite[Lemma~3.3]{Mourrain97} we have $w \le
N_i-1$. Then, we let
$$\pi =\sum_{\mu \in S_i,\ |\mu|=w}P_{i,\mu}(\bu_i) X_1^{\mu_1} \cdots
X_n^{\mu_n}$$ be the {\em symbol} of $\lambda$; by construction,
this is a non-zero polynomial. In the following paragraphs, we will
need the next easy lemma.

\begin{Lemma}\label{lemma:symbol0}
	Fix $i$ in $\{1,\dots,d\}$. For a generic choice of $\lambda$ in ${\rm
		hom}_\Kbar(Q_i,\Kbar)$, and of $t_1,\dots,t_n$ in $\Kbar{}^n$,
	$\pi_i(t_1,\dots,t_n)$ is non-zero.
\end{Lemma}
\begin{proof}
	Let $\Omega$ be the maximum of all $|\mu|=\mu_1+\cdots+\mu_n$ for
	$\mu=(\mu_1,\dots,\mu_n)$ in $S_i$, and define 
	$$\Pi =\sum_{\mu \in S_i,\ |\mu|=\Omega}P_{i,\mu} X_1^{\mu_1} \cdots
	X_n^{\mu_n} \in \Kbar[U_{i,1},\dots,U_{i,D_i},X_1,\dots,X_n];$$ this
	is by construction a non-zero polynomial.  Thus, for a generic
	choice of $\bu_i=(u_{i,1},\dots,u_{i,D_i})$, that define a linear form
	$\lambda$ in ${\rm hom}_\Kbar(Q_i,\Kbar)$ as in~\eqref{ell_param},
	and of $t_1,\dots,t_n$ in $\Kbar{}^n$, the value
	$\Pi(u_{i,1},\dots,u_{i,D_i},t_1,\dots,t_n)$ is non-zero. As a
	result, the symbol of such a linear form $\lambda$ is $\pi =\sum_{\mu \in
		S_i,\ |\mu|=\Omega}P_{i,\mu}(\bu_i) X_1^{\mu_1} \cdots X_n^{\mu_n},$
	and $\pi(t_1,\dots,t_n)$ is then non-zero.
\end{proof}



Finally, we say a word about global objects.  Fix a linear form $\ell:
Q \to \K$. By the Chinese Remainder Theorem, there exist unique
$\ell_1,\dots,\ell_d$, with $\ell_i$ in ${\rm hom}_\Kbar(Q_i,\Kbar)$
for all $i$, such that the extension $\ell_\Kbar: Q\otimes_\K \Kbar
\to \Kbar$ decomposes as $\ell_\Kbar = \ell_1 + \cdots + \ell_d$. We
call {\em support} of $\ell$ the subset $\mathfrak{S}$ of
$\{1,\dots,d\}$ such that $\ell_i$ is non-zero exactly for $i$ in
$\mathfrak{S}$.  As a consequence, for all $f$ in $Q$, we have
\begin{align}\label{eq:fui}
\ell(f) &= \ell_1(f) + \cdots + \ell_d(f)\nonumber\\
&=  \sum_{i \in \mathfrak{S}} \ell_i(f).
\end{align}
For $i$ in $\mathfrak{S}$, we denote by $w_i$ and $\pi_i$ respectively
the order and the symbol of $\ell_i$. For such a subset
$\mathfrak{S}$, we also write $Q_\mathfrak{S}=\prod_{i \in
	\mathfrak{S}} Q_i$ and $V_\mathfrak{S}=\cup_{i \in \mathfrak{S}}
\{\balpha_i\}$.


\subsubsection{A fundamental formula.}  The following lemma 
gives an explicit form for a generating series of the form $\sum_{\ell
	\ge 0} \ell(v t^i)T^\ell$, for a linear form $\ell:Q \to \K$. A
slightly less precise version of it is in~\cite{BoSaSc03}.

\begin{Lemma}\label{lemma:formula}
	Let $\ell$ be in ${\rm hom}_\K(Q,\K)$, with support $\mathfrak{S}$,
	and let $\{\pi_i \mid i \in \mathfrak{S}\}$ and $\{w_i \mid i \in
	\mathfrak{S}\}$ be as above.
	
	Let $t=t_1 X_1 + \cdots +t_n X_n$, for some $t_1,\dots,t_n$ in $\K$
	and let $v$ be in $\K[X_1,\dots,X_n]$. Then, we have the equality
	\begin{align}\label{eq:sumgenseries}
	\sum_{s \ge 0} \ell(v t^s)/T^{s+1} =
	\sum_{i \in \mathfrak{S}} \frac{
		v(\balpha_i)\, w_i!\, \pi_{i}(t_1,\dots,t_n)
		+ (T-t(\balpha_i))A_{v,i}}
	{(T-t(\balpha_i))^{w_{i}+1}},    
	\end{align}
	for some polynomials $\{A_{v,i} \in \Kbar[T] \mid i \in \mathfrak{S}\}$ (that
	depend on the choice of $v$), with $A_{v,i}$ of degree less than $w_i$ for all $i$ in
	$\mathfrak{S}$.
\end{Lemma}
\begin{proof}
	Take $v$ and $t$ as above. Consider first
	an operator of the form $f \mapsto \frac{ \partial^{|\mu|}  f}
	{\partial X_1^{\mu_1} \cdots \partial X_n^{\mu_n}}$, where 
	we write $|\mu|=\mu_1+\cdots+\mu_n$. Then, we have
	the following generating series identities, with coefficients in 
	$\K(X_1,\dots,X_n)$:
	\begin{align*}
	\sum_{s \ge 0} 
	\frac{ \partial^{|\mu|} ( v t^s )} {\partial X_1^{\mu_1} \cdots
		\partial X_n^{\mu_n}}
	\frac{1}{T^{s+1}} 
	&=  \sum_{s \ge 0} 
	\frac{ \partial^{|\mu|} (v t^s/T^{s+1})} {\partial X_1^{\mu_1} \cdots
		\partial X_n^{\mu_n}}\\
	&=  
	\frac{ \partial^{|\mu|} } {\partial X_1^{\mu_1} \cdots
		\partial X_n^{\mu_n}}
	\left (\sum_{s \ge 0} v t^s/T^{s+1}\right ) \\
	&= \frac{ \partial^{|\mu|} } {\partial X_1^{\mu_1} \cdots
		\partial X_n^{\mu_n}}
	\left (\frac v{T-t} \right ) \\
	&= v\, |\mu|!\, \prod_{1 \le k \le n} 
	\left (\frac{ \partial t} {\partial X_k} \right)^{\mu_k}
	\frac {1}{(T-t)^{|\mu|+1}} + \frac{P_{|\mu|}(\bX,T)}{(T-t)^{|\mu|}} + \cdots + \frac{P_{1}(\bX,T)}{(T-t)}\\
	&= v\, |\mu|!\, \prod_{1 \le k \le n} 
	t_k^{\mu_k}
	\frac {1}{(T-t)^{|\mu|+1}} + \frac{P(\bX,T)}{(T-t)^{|\mu|}},
	\end{align*}
	for some polynomials $P_1,\dots,P_{|\mu|},P$ in $\K[\bX,T]$ that
	depend on the choices of $\mu$, $v$ and $t$, with $\deg(P_i,T) < i$
	for all $i$ and thus $\deg(P,T) < |\mu|$.
	
	Take now a linear combination of such operators, such as 
	$f \mapsto \sum_{\mu \in R} c_\mu \frac{ \partial^{|\mu|}  f } {\partial X_1^{\mu_1} \cdots
		\partial X_n^{\mu_n}}$. The corresponding generating series
	becomes
	\begin{align*}
	\sum_{s \ge 0} 
	\sum_{\mu \in R} c_\mu \frac{ \partial^{|\mu|} ( v t^s )} {\partial X_1^{\mu_1} \cdots
		\partial X_n^{\mu_n}}
	\frac{1}{T^{s+1}}
	&=
	v\,\sum_{\mu \in R} c_\mu
	|\mu|!\, \prod_{1 \le k \le n} 
	t_k^{\mu_k}
	\frac {1}{(T-t )^{|\mu|+1}} +\sum_{\mu \in R} \frac{P_\mu(\bX,T)}{(T-t)^{|\mu|}},
	\end{align*}
	where each $P_\mu$ has degree less than $|\mu|$ in $T$.
	Let $w$ be the maximum of all $|\mu|$ for $\mu$ in $R$. We can rewrite 
	the above as
	\begin{align*}
	v\, w! 
	\sum_{\mu \in R, |\mu|=w} c_\mu
	\, \prod_{1 \le k \le n} 
	t_k^{\mu_k}
	\frac {1}{(T-t )^{w+1}}
	+ \frac{A(\bX,T)}{(T-t )^{w}},
	\end{align*}
	for some polynomial $A$ of degre less than $w$ in $T$. If we let 
	$\pi =\sum_{\mu \in R,\ |\mu|=w} c_{\mu} X_1^{\mu_1} \cdots
	X_n^{\mu_n}$, this becomes
	\begin{align*}
	\sum_{s \ge 0} 
	\sum_{\mu \in R} c_\mu \frac{ \partial^{|\mu|} ( v t^s )} { X_1^{\mu_1} \cdots
		X_n^{\mu_n}}
	\frac{1}{T^{s+1}} 
	&=
	v\, w! \,  \pi(t_1,\dots,t_n)
	\frac {1}{(T-t )^{w+1}}
	+ \frac{A(\bX,T)}{(T-t )^{w}}.
	\end{align*}
	Applying this formula to the sum in~\eqref{eq:fui}, we obtain the
	claim in the lemma.
\end{proof}

\noindent 
The most useful consequence of the previous lemma is the following
interpolation formula, where we fix a subset $\mathfrak{S}$ of 
$\{1,\dots,d\}$. The mapping $t:V_\mathfrak{S} \to \Kbar$
defined by $\balpha_i \mapsto t(\balpha_{i})$ plays a special role in
the formula in the lemma; this leads us to the following definitions.
\begin{itemize}
	\item We consider $\ell$ and $t$ as in Lemma~\ref{lemma:formula}, such
	that $\ell$ has support $\mathfrak{S}$.
	\item $\mathfrak{T}$ is the subset of $\mathfrak{S}$ consisting of
	all indices $i$ such that 
	\begin{itemize}
		\item $\pi_i(t_1,\dots,t_n)$ is non-zero;
		\item $t(\balpha_{i'}) \ne t(\balpha_i)$ for $i' \ne i$ in $\mathfrak{S}$.
	\end{itemize}
	\item $\{r_1,\dots,r_c\}$ are the pairwise distinct values taken by $t$ on
	$V_\mathfrak{S}$, for some $c \le |\mathfrak{S}|$.
	\item $\mathfrak{t}$ is the set of all indices $j$ in
	$\{1,\dots,c\}$ such that
	\begin{itemize}
		\item the fiber $t^{-1}(r_j) \subset V_{\mathfrak{S}}$ contains a single
		point, written $\balpha_{\sigma_j}$;
		\item the point $\balpha_{\sigma_j}$ is in $\mathfrak{T}$
		(equivalently,  $\pi_{\sigma_j}(t_1,\dots,t_n)$ is non-zero).
	\end{itemize}
	Remark that $j \mapsto \sigma_j$ induces a one-to-one correspondence
	between  $\mathfrak{t}$ and  $\mathfrak{T}$.
\end{itemize}

\begin{Lemma} \label{lemma:anyv}
	Let $\ell$, $t$ and all other notation be as above. Let further
	$M$ be the minimal polynomial of $t$ in $Q_\mathfrak{S}$. Suppose that 
	$M$ is also the minimal polynomial of the sequence $(\ell(t^s))_{s \ge 0}$,
	then there exist non-zero constants $\{c_j \mid j \in
	\mathfrak{t}\}$  such that for $v$ in
	$\K[X_1,\dots,X_n]$,
	$$\Omega((\ell(v t^s))_{s\ge0},M) = c_{j} v(\balpha_{\sigma_j}) \quad \text{for all $j$ in $\mathfrak{t}$.}$$
\end{Lemma}
\begin{proof}
	For $j=1,\dots,c$, we write $T_j$ for the set of all indices $i$ in
	$\mathfrak{S}$ such that $t(\balpha_{i})=r_j$; the sets
	$T_1,\dots,T_c$ form a partition of $\mathfrak{S}$. When $T_j$ has
	cardinality~$1$, we thus have $T_j=\{\sigma_j\}$.
	
	Take an arbitrary $v$ in $\K[X_1,\dots,X_n]$ and let us
	collect terms in~\eqref{eq:sumgenseries} as
	\begin{align*}
	\sum_{s \ge 0} \ell(v t^s)\frac{1}{T^{s+1}} =&
	\sum_{j \in \{1,\dots,c\}}
	\sum_{i \in T_j} \frac{
		v(\balpha_{i})   w_{i}!\, \pi_{i}(t_1,\dots,t_n)
		+ (T-r_{j} )A_{v,i}}
	{(T-r_{j} )^{w_{i}+1}}\\
	=&
	\sum_{j \in \mathfrak{s}}
	\frac{
		v(\balpha_{\sigma_{j}})  w_{\sigma_j}!\, \pi_{\sigma_j}(t_1,\dots,t_n)
		+ (T-r_{j}  )A_{v,\sigma_j} }
	{(T-r_{j} )^{w_{\sigma_j}+1}}\\
	&+
	\sum_{j \in \{1,\dots,c\}-\mathfrak{s}}
	\frac{   \sum_{i \in T_j} \Big( \big[
		v(\balpha_{i})   w_{i}!\, \pi_{i}(t_1,\dots,t_n)
		 + (T-r_{j}  )A_{v,i} \big ]
		(T-r_{j} )^{y_j-(w_i+1)}\Big)}
	{(T-r_{j} )^{y_j}},
	\end{align*}
	where $y_j$ is the maximum of all $w_i$ for $i$ in $T_j$.  Remark that
	for $v=1$, our condition that $\pi_i$ is non-zero for $i$ in
	$\mathfrak{T}$ implies that in the second line, together with our
	assumption on the characteristic of $\K$, imply that all terms in the
	first sum are non-zero and in reduced form.  
	
	After simplyfing terms in the second sum, we can rewrite the
	expression above as
	\begin{align*}
	\sum_{s \ge 0} \ell(v t^s)\frac{1}{T^{s+1}} =&
	\sum_{j \in \mathfrak{t}} \frac{
		v(\balpha_{\sigma_{j}})   w_{\sigma_{j}}!\, \pi_{{\sigma_{j}}}(t_1,\dots,t_n)
		+ (T-r_{j} )A_{v,\sigma_{j}}}
	{(T-r_{j} )^{w_{\sigma_{j}}+1}}  
	+\sum_{j \in  \{1,\dots,c\}-\mathfrak{s}}
	\frac{D_{v,j}}
	{(T-r_{j})^{z_{v,j}}},
	\end{align*}
	for some positive integers $\{z_{v,j} \mid j\in
	\{1,\dots,c\}-\mathfrak{s}\}$ and polynomials $\{D_{v,j} \mid j\in
	\{1,\dots,c\}-\mathfrak{s}\}$ such that for all $j$ in $
	\{1,\dots,c\}-\mathfrak{s}$, we have $\deg(D_{v,j}) < z_{v,j}$ and
	$\gcd(D_{v,j}, (T-r_{j} ))=1$; the integers $z_{v,j}$ are uniquely
	determined by these conditions, except if $r_{j}=0$, in which case we
	set $z_{v,j}=\deg(D_{v,j})+1$. Some of the polynomials $D_{v,j}$ may
	vanish, so we let $\mathfrak{u}_v \subset \{1,\dots,c\}-\mathfrak{s}$
	be the set of all $j$ for which this is not the case. 
	We then arrive at our final form for this sum, namely
	\begin{align}\label{eq:sumgenseries_collect1}
	\sum_{s \ge 0} \ell(v t^s) \frac{1}{T^{s+1}} =&
	\sum_{j \in \mathfrak{t}} \frac{
		v(\balpha_{\sigma_{j}})   w_{\sigma_{j}}!\, \pi_{{\sigma_{j}}}(t_1,\dots,t_n)
		+ (T-r_{j}  )A_{v,\sigma_{j;}}}
	{(T-r_{j} )^{w_{\sigma_{j}}+1}}  
	+\sum_{j \in  \mathfrak{u}_v}
	\frac{D_{v,j}}
	{(T-r_{j} )^{z_{v,j}}},
	\end{align}
	where all terms in the second sum are non-zero and in reduced form
	(and similarly for the first sum, for $v=1$).
	This implies that the minimal
	polynomial of the sequence $(\ell(vt^s))_{s \ge 0}$ is 
	$$M_v=\prod_{j \in \mathfrak{t}} (T-r_{j})^{\zeta_j} \prod_{j
		\in \mathfrak{u}_v} (T-r_{j})^{z_{v,j}},$$
	for some integers $\{\zeta_j \le w_{\sigma_{j}}+1 \mid j \in \mathfrak{t}\}$; for $v=1$, 
	we actually have $\zeta_j = w_{\sigma_{j}}+1$ for all such~$j$.
	
	Now, for $v=1$, we assume that the minimal polynomial of the sequence
	$(\ell(t^s))_{s \ge 0}$ is the minimal polynomial $M$ of $t$ in
	$Q_\mathfrak{S}$.  Writing $\mathfrak{u}=\mathfrak{u}_1$ and
	$z_k=z_{1,k}$ for all $k$ in $\mathfrak{u}$, we can thus write it as
	$$M=\prod_{j \in \mathfrak{t}} (T-r_{j})^{w_{\sigma_{j}}+1} \prod_{j
		\in \mathfrak{u}} (T-r_{j})^{z_{j}}.$$ Since it is the minimal
	polynomial of $t$ in $Q_\mathfrak{S}$, it also cancels the sequence
	$(\ell(v t^s))_{s \ge 0}$ for any $v$, so that for all $v$,
	$\mathfrak{u}_v$ is contained in $\mathfrak{u}$ and $M_v$ divides $M$.
	Remark also that the integer $\delta=\deg(M)$ is given by
	$$\delta=\sum_{j \in \mathfrak{t}} (w_{\sigma_j}+1)
	+\sum_{j\in\mathfrak{u}} z_{j}.$$ 
	
	For $v$ arbitrary in $\K[X_1,\dots,X_n]$, since  $M$ cancels the sequence
	$(\ell(v t^s))_{s \ge 0}$, $\Omega((\ell(v t^s))_{s \ge 0} ,M)$ in the statement of
	the lemma is indeed a polynomial of degree less than $\delta$ (this
	proves our first claim). We can then rewrite the
	sum in~\eqref{eq:sumgenseries_collect1} as $ \sum_{s \ge 0} \ell(v
	t^s)/T^{s+1} =\Omega((\ell(v t^s))_{s \ge 0} ,M)/M$, with
	\begin{align*}
	\Omega((\ell(v t^s))_{s \ge 0} ,M)=&\sum_{j \in \mathfrak{t}} \Big(\big[
	v(\balpha_{\sigma_{j}})  w_{\sigma_j}!\, \pi_{\sigma_j}(t_1,\dots,t_n)
	+ (T-r_j)\tilde A_{v,\sigma_j}\big]
	\prod_{\iota \in \mathfrak{t}-\{j\}}(T-r_{\iota} )^{w_{\sigma_{\iota}}+1}
	\Big)\prod_{j \in \mathfrak{u}}(T-r_j )^{z_j}
	\\
	&+
	\Big(\prod_{j \in \mathfrak{t}}(T-r_{j} )^{w_{\sigma_j}+1}\Big)
	\sum_{j \in \mathfrak{u}_v} \Big (D_{v,j}
	(1-r_j T)^{z_{j}-z_{v,j}}
	\prod_{\iota \in \mathfrak{u}-\{j\}}(T-r_{\iota} )^{z_{\iota}}\Big),
	\end{align*}
	In particular, 
	for $k$ in $\mathfrak{t}$,
	the value $\Omega((\ell(v t^s))_{s \ge 0} ,M)(r_k)$ is 
	\begin{align*}
	\Omega((\ell(v t^s))_{s \ge 0} ,M)(r_k)&= v(\balpha_{\sigma_{k}}) w_{\sigma_k}!\, \pi_{\sigma_k}(t_1,\dots,t_n)
	\prod_{\iota \in \mathfrak{t}-\{k\}}(r_\iota-r_{k} )^{w_{\sigma_{\iota}}+1}
	\prod_{j \in \mathfrak{u}}(r_j-r_{k} )^{z_{k}}\\
	&= v(\balpha_{\sigma_{k}}) c_{k},
	\end{align*}
	with 
	$$c_{k}=
	w_{\sigma_k}!\, \pi_{\sigma_k}(t_1,\dots,t_n)
	\prod_{\iota \in \mathfrak{t}-\{k\}}(r_\iota-r_{k} )^{w_{\sigma_{\iota}}+1}
	\prod_{j \in \mathfrak{u}}(r_j-r_{k} )^{z_{k}}$$
	for $k$ in $\mathfrak{t}$. This is a non-zero constant, independent 
	of $v$, which finishes the proof of the lemma.
\end{proof}

As an application, the following algorithm shows how to compute a
zero-dimensional parametrization of $V_{\mathfrak{S}}$.

\begin{algorithm}[H]
	\caption{$\mathsf{ParametrizationGeneric}(\ell,t)$}
	~\\
	{\bf Input:} \vspace{-0.5em}
	\begin{itemize}\setlength\itemsep{0em}
		\item  a linear form $\ell$ over $Q_\mathfrak{S}$
		\item $t=t_1 X_1 + \cdots + t_n X_n$
	\end{itemize}
	{\bf Output:} polynomials $(P,V_1,\dots,V_n)$
	\begin{enumerate}\setlength\itemsep{0em}
		\item let $M$ be the minimal polynomial of the sequence $(\ell(t^s))_{s \ge 0}$ and let $\delta$ be its degree
		\item let $P$ be the squarefree part of $M$
		\item let $C_1 = \Omega((\ell(t^s))_{s\ge0} ,M)$
		\item \textbf{for} $i=1,\dots,n$ \textbf{do}
		\begin{enumerate}
			\item let $C_{X_i} = \Omega((\ell(X_i t^s))_{s\ge0}, M)$ 
		\end{enumerate}
		\item \textbf{return} $(P, C_{X_1}/ C_1 \bmod P, \dots, C_{X_n}/ C_{1} \bmod P)$
	\end{enumerate}
	\label{algo:para2}
\end{algorithm}

\begin{Lemma}
	Suppose that $\ell$ is a generic element of ${\rm
		hom}_{\Kbar}(Q_\mathfrak{K},\Kbar)$ and that $t$ is a generic
	linear form. Then the output $((P,V_1,\dots,V_n),t)$ of
	$\mathsf{Parametrization}(\ell,t)$ is a zero-dimensional
	parametrization of $V_{\mathfrak{S}}$.
\end{Lemma}
\begin{proof}
	A generic choice of $t$ separates the points of $V_{\mathfrak{S}}$,
	and we saw in Lemma~\ref{lemma:symbol0} that for a generic choice of
	$\ell$, $\pi_i(t_1,\dots,t_n)$ vanishes for no $i$ in
	$\mathfrak{S}$.  As a result, $\mathfrak{T}=\mathfrak{S}$.  Besides,
	we recall that for a generic $\ell$ in ${\rm
		hom}_{\Kbar}(Q_\mathfrak{S},\Kbar)$, the minimal polynomials of
	$(\ell(t^s))_{s \ge 0}$ and of $t$ are the same
	(Lemma~\ref{lemma:minpoly}).
	
	Thus, the polynomial $M$ we compute at step 1 is indeed the minimal
	polynomial of $t$, and we can apply the previous lemma, and for 
	any root $r_j$ of $P$, and $i=1,\dots,n$, we have
	$$\frac{ C_{X_i}(r_j)}{ C_1(r_j)}  = \frac{c_j \alpha_{\sigma_j,i}}{c_j} = \alpha_{\sigma_j,i},$$
	so that $ C_{X_i}/ C_1 \bmod P$ is the $i$th polynomial in
	the zero-dimensional parametrization of $V$ corresponding to $t$.
\end{proof}

We demonstrate how this algorithm works through a small example. Let 
$I = \langle (X_1-1)(X_2-2),(X_1-3)(X_2-4)\rangle \subset
GF(101)[X_1,X_2]$, then
clearly $V(I) = \{ (1,4),(3,2) \}$ and $X_1$ separates the
points of $V$. We choose a random linear form 
$$\ell: f \in I \mapsto \mathbb{N},\;\ell(f) = 17 f(1,4) + 33 f(3,2)$$
Then we have
\begin{align*}
\ell(X_1^i) &= 17 \cdot 1^i + 33 \cdot 3^i\\
\ell(X_2X_1^i) &= 17 \cdot 4 \cdot 1^i + 33 \cdot 2 \cdot 3^i
\end{align*} 
We define an infinite series for each sequence
\begin{align*}
Z_1 = \sum_{i = 0}^{\infty} \ell(X^i_1) / T^{i+1}
&= \frac{17}{T-1} + \frac{33}{T-3}
= \frac{17(T-3)+33(T-1)}{(T-1)(T-3)} \\
Z_2 = \sum_{i=0}^{\infty} \ell(X_2X^i_1)/T^{i+1} 
&= \frac{17\cdot 4}{T-1} + \frac{33 \cdot 2}{t-3}
= \frac{17\cdot 4 (T-3) + 33\cdot 2(T-1)}{(T-1)(T-3)}
\end{align*}
$Z_1$ and $Z_2$ have a common denominator $P = (t-1)(t-3)$,
whose roots are the coordinates of $X_1$ in $V(I)$. Now, let
\begin{align*}
R_2 
&=\frac{\Omega((\ell(X_2X^i_1))_{i\ge 0},P)}{\Omega((\ell(X^i_1))_{i\ge 0},P)} \mod P\\
&= 
\frac{17\cdot 4 (T-3) + 33\cdot 2(T-1)}{17(T-3)+33(T-1)} \mod P\\
&=\frac{4 (T-3) + 2(T-1)}{(T-3)+(T-1)} \mod P
\end{align*}
Now, $ R_2(1) = 4$ and $R_2(3) = 2$ as needed.

\subsubsection{Decomposition of linear forms.}

In this paragraph, we work over the whole $V$ (so
$\mathfrak{S}=\{1,\dots,d\}$).  Specializing our previous discussion
to the case $t=X_1$, we let $r_1,\dots,r_c$ be the pairwise distinct
values taken by $X_1$ on $V$, for some $c \le d$.  For
$j=1,\dots,c$, we write $T_j$ for the set of all indices $i$ in
$\{1,\dots,d\}$ such that $\alpha_{i,1}=r_j$; the sets $T_1,\dots,T_c$
form a partition of $\{1,\dots,d\}$. When $T_j$ has cardinality~$1$,
we denote it as $T_j=\{\sigma_j\}$, for some index $\sigma_j$ in
$\{1,\dots,d\}$, so that $\alpha_{\sigma_j,1}=r_j$.

For $i=1,\dots,d$, let us write $\nu_i$ for the degree of the minimal
polynomial of $X_1$ in $Q_i$; thus, this polynomial is
$(T-\alpha_{i,1})^{\nu_i}$. For $j$ in $\{1,\dots,c\}$, we define
$m_j$ as the maximum of all $\nu_i$, for $i$ in~$T_j$. As a result, the minimal
polynomial of $X_1$ in $\prod_{j \in T_j} Q_j$ is 
$(T-r_j)^{m_j}$, and the minimal polynomial of $X_1$ in $Q$ is
$M=\prod_{j \in \{1,\dots,c\}} (T-r_j)^{m_j}$.

Recall that a linear form $\ell: Q \to \Kbar$ can be written uniquely
as $\ell=\sum_{i\in \{1,\dots,d\}} \ell_i$, with $\ell_i:Q_i \to
\Kbar$; collecting terms, $\ell$ may also be written as $\ell=\sum_{j
	\in \{1,\dots,c\}} \lambda_j$, with $\lambda_j=\sum_{i \in T_j}
\ell_i$.  Given such an $\ell$, we first explain how to compute values
of the form $\lambda_j(1)$. We will do this for some values of $j$
only, namely those $j$ for which $m_j=1$.

\begin{Lemma}\label{lemma:valuelambda}
	Let $\ell$ be in ${\rm hom}_\K(Q,\K)$, let $M$ be the minimal
	polynomial of $X_1$ in $Q$, let $\delta$ be its degree and 
	let $B=T^{\delta}M(1/T)$. Then, the following holds:
	\begin{itemize}
		\item the power series $A = \Big(\sum_{s \ge 0}
		\ell(X_1^s)T^s\Big)B$ is actually a polynomial of degree less than~$\delta$;
		\item the polynomial $\tilde A = T^{\delta-1} A(1/T)$ satisfies
		$$\tilde A(r_j) = \lambda_j(1) M'(r_j) \quad \text{for all $j$ such that $m_j=1$.}$$
	\end{itemize}
\end{Lemma}
\begin{proof}
	Let $\mathfrak{e}$ be the set of all indices $j$ in $\{1,\dots,c\}$
	such that $m_j=1$, and let $\mathfrak{f}=\{1,\dots,c\}-\mathfrak{e}$;
	this definition allows us to split the sum as
	\begin{align*}
	\sum_{s \ge 0} \ell(X_1^s) T^s  
	&= \sum_{j \in \{1,\dots,c\}}\sum_{i\in T_j} 
	\sum_{s \ge 0}\ell_i(X_1^s)T^s  \\
	&=\sum_{j \in \mathfrak{e}}\sum_{i\in T_j}\sum_{s \ge 0}  \ell_i(X_1^s)T^s +
	\sum_{j \in \mathfrak{f}}\sum_{i\in T_j}\sum_{s \ge 0}  \ell_i(X_1^s)T^s.
	\end{align*}
	Using Lemma~\ref{lemma:formula} with $t=X_1$ and $v=1$, any sum $\sum_{s \ge 0} \lambda_j(X_1^s)T^s$ 
	in the second summand
	can be rewritten as 
	$$\frac{C_j}{(1-r_j T)^{v_j}},$$
	for some integer $v_j$, and for some polynomial $C_j$ of degree less than
	$v_j$. Next, take $j$ in $\mathfrak{e}$. Since $m_j=1$, $\nu_i=1$ for all $i$ in $T_j$,
	so that
	each such $\ell_i$ takes the form 
	$$\ell_i: f \mapsto (\Lambda_{i}(f))(\balpha_i),$$ where $\Lambda_{i}$
	is a differential operator that does not involve $\partial/\partial
	X_1$. Since all terms of positive order in $\Lambda_i$ involve one of
	$\partial/\partial X_2,\dots,\partial/\partial X_n$, they cancel
	$X_1^s$ for $s\ge 0$. Thus, $\ell_i(X_1^s)$ can be rewritten 
	as $\ell_{i,1} \alpha_{i,1}^s$, for some constant $\ell_{i,1}$,
	and the generating series of these terms is 
	$$\frac {\ell_{i,1}}{1-\alpha_{i,1}T}=\frac {\ell_{i,1}}{1-r_j T}.$$
	Remarking  that we can write $\ell_{i,1}=\ell_i(1)$,
	altogether, the sum in question can be written
	\begin{align*}
	\sum_{s \ge 0} \ell(X_1^s) T^s  
	&=\sum_{j \in \mathfrak{e}} 
	\frac{ \sum_{i\in T_j}  \ell_{i}(1) }{1-r_j T}
	+ \sum_{j \in \mathfrak{f}} \frac{D_j}{(1-r_j T)^{x_j}}\\
	&= \sum_{j \in \mathfrak{e}} 
	\frac{ \lambda_j(1) }{1-r_j T}
	+ \sum_{j \in \mathfrak{f}} \frac{D_j}{(1-r_j T)^{x_j}}
	\end{align*}
	for some integers $\{x_j \mid j \in b\}$ such that $\deg(D_j) < x_j$
	holds, and with $D_j$ and $1-r_j T$ coprime; if $r_j=0$, we take
	$x_j=\deg(D_j)+1$. In particular, the minimal polynomial of
	$(\ell(X_1^s))_{s\ge 0}$ is $N=\prod_{j\in \mathfrak{e}}(T-r_j)
	\prod_{j \in \mathfrak{f}}(T-r_j)^{x_j}$.
	
	On the other hand, the minimal polynomial $M$ of $X_1$ can be rewritten as $M=\prod_{j\in
		\mathfrak{e}}(T-r_j) \prod_{j \in \mathfrak{f}}(T-r_j)^{m_j}$, so
	that $\delta=\sum_{j \in \mathfrak{e}} 1 + \sum_{j\in \mathfrak{f}}
	m_j$ and $B=\prod_{j\in \mathfrak{e}}(1-r_j T) \prod_{j \in
		\mathfrak{f}}(1-r_j T)^{m_j}$. The minimal polynomial
	of the sequence $\ell(X_1^s)$ divides $M$, so that $x_j \le m_j$ holds for all $j$ in
	$\mathfrak{f}$.
	As a result,  $A =\big ( \sum_{s \ge 0} \ell(X_1^s) T^s \big ) B$ is 
	indeed a polynomial of degree less than $\delta$, given by
	\begin{align*}
	A=&
	\sum_{j \in \mathfrak{e}}
	\Big(
	\lambda_j(1) \prod_{\iota \in \mathfrak{e}-\{j\}}(1-r_\iota T)\Big)
	\Big(\prod_{j \in \mathfrak{f}}(1-r_j T)^{m_j} \Big)\\
	&+
	\sum_{j\in \mathfrak{f}}
	\Big(D_j (1-r_j T)^{m_j-x_j}
	\prod_{\iota \in \mathfrak{f}-\{j\}}(1-r_j T)^{m_\iota}\Big)
	\Big(\prod_{j\in \mathfrak{e}} (1-r_jT) \Big).
	\end{align*}
	The reciprocal polynomial $\tilde A=T^{\delta-1} A(1/T)$ is then
	\begin{align*}
	\tilde  A=&
	\sum_{j \in \mathfrak{e}}
	\Big(
	\lambda_j(1) \prod_{\iota \in \mathfrak{e}-\{j\}}(T-r_\iota)\Big)
	\Big(\prod_{j \in \mathfrak{f}}(T-r_j)^{m_j} \Big)\\
	&+
	\sum_{j\in \mathfrak{f}}
	\Big(\tilde D_j (T-r_j)^{m_j-x_j}
	\prod_{\iota \in \mathfrak{f}-\{j\}}(T-r_j)^{m_\iota}\Big)
	\Big(\prod_{j\in \mathfrak{e}} (T-r_j) \Big),
	\end{align*}
	with $\tilde D_j = T^{x_j-1} D_j(1/T)$ for all $j$ in $\mathfrak{f}$.
	This implies that $$\tilde A(r_k) =\lambda_k(1) 
	\prod_{\iota \in \mathfrak{e}-\{k\}}(r_k-r_\iota)
	\prod_{j \in \mathfrak{f}}(r_k-r_j)^{m_j} = \lambda_k(1) M'(r_k)$$ 
	holds for all $k$ in $\mathfrak{e}$.
\end{proof}

We then show how to use this result to avoid (as much as possible)
using a generic linear form $t=t_1 X_1 + \cdots + t_n X_n$, and how to
use (say) $X_1$ instead to compute a zero-dimensional parametrization
of a subset of $V$; this is motivated by the fact that the
multiplication matrix by $X_1$ is expected to be sparser than that of
$t$ (since the matrix of $t$ is a combination of those of
$X_1,\dots,X_n$), sometimes by a substantial amount. Of course, there
is no guarantee that $X_1$ is a separating element for $V$. As a
result, we will compute a decomposition of $V$ into two components
$V'$ and $V''$; $X_1$ will be a separating element for $V'$, whereas
we will use a generic linear form to describe $V''$.

More precisely, we characterize the set $V'$ mentioned above as follows: for $i$ in
$\{1,\dots,d\}$, $\balpha_i$ is in $V'$ if and only if:
\begin{itemize}
	\item for $i'$ in $\{1,\dots,d\}$, with $i'\ne i$, $\alpha_{i',1} \ne
	\alpha_{i,1}$;
	\item $Q_i$ is a reduced algebra (equivalently, $I_i$ is radical).
\end{itemize}
We denote by $\mathfrak{A}\subset \{1,\dots,d\}$ the set of
corresponding indices $i$, and we let
$\mathfrak{B}=\{1,\dots,d\}-\mathfrak{a}$, so that we have
$V'=V_{\mathfrak{A}}$ and $V''=V_{\mathfrak{B}}$.  Remark that $X_1$
is a separating element for $V'$.

Correspondingly, we define $\mathfrak{a}$ as the set of all indices
$j$ in $\{1,\dots,c\}$ such that $\sigma_j$ is in $\mathfrak{A}$. In
other words, $j$ is in $\mathfrak{a}$ if and only if $T_j$ has
cardinality $1$ and $Q_{\sigma_j}$ is reduced.  The algorithm in this
paragraph will compute a zero-dimensional parametrization of
$V_{\mathfrak{A}}$; we use the following lemma to perform this
decomposition of $V$.

\begin{Lemma}\label{lemma:acb2}
	Let $j$ be in $\{1,\dots,c\}$ such that $m_j=1$, let $\lambda$ be a
	linear form over $\prod_{i \in T_j} Q_i$ and let $t=t_2 X_2
	+ \cdots + t_n X_n$. Define constants $a,b,c$ in $\Kbar$ by
	$$a=\lambda(1),\quad b=\lambda(t),\quad c=\lambda(t^2).$$
	Then, $j$ is in $\mathfrak{a}$
	if and only if, for a generic choice of $\lambda$ and $t$, $ac=b^2$.
\end{Lemma}
\begin{proof}
	The assumption that $m_j=1$ means that for all $i$ in $T_j$,
	$\nu_i=1$. The linear 
	form $\lambda$ can be uniquely written as a sum $\lambda=\sum_{i \in T_j}
	\ell_i$, where each $\ell_i$ is in ${\rm hom}_\Kbar(Q_i,\Kbar)$.
	The fact that all $\nu_i$ are equal to $1$ then implies that each $\ell_i$ takes the form 
	$$\ell_i: f \mapsto (\Lambda_{i}(f))(\balpha_i),$$
	where $\Lambda_{i}$ is a differential operator that does not 
	involve $\partial/\partial X_1$. Thus, as in~\eqref{ell_param}, we can write a general
	$\Lambda_i$ of this form as
	$$\Lambda_i: f \mapsto u_{i,1} f + \sum_{2 \le r \le n}
	P_{i,r}(u_{i,2},\dots,u_{i,D_i}) \frac{\partial}{\partial X_j} f +
	\sum_{2 \le r \le s \le n} P_{i,r,s}(u_{i,2},\dots,u_{i,D_i})
	\frac{\partial^2}{\partial X_j\partial X_k} f +
	\tilde\Lambda_i(f),$$ where all terms in $\tilde \Lambda_i$ have
	order at least $3$, $\bu_i=(u_{i,1},\dots,u_{i,D_i})$ are parameters and
	$(P_{i,r})_{2 \le r \le n}$ and $(P_{i,r,s})_{2 \le r \le s \le n}$
	are linear forms in $u_{i,2},\dots,u_{i,D_i}$.
	We obtain
	\begin{align*}
	\Lambda_i(1)   &= u_{i,1} \\
	\Lambda_i(t)   &= u_{i,1} t +\sum_{2 \le r \le n}P_{i,r}t_r \\
	\Lambda_i(t^2) &= u_{i,1} t^2  +2 t \sum_{2 \le r \le n}P_{i,r}t_r   
	+ 2\sum_{2 \le r \le s \le n} P_{i,r,s}t_rt_s,
	\end{align*}
	which gives
	\begin{align*}
	a  &= \sum_{i\in T_j}u_{i,1} \\
	b  &= \sum_{i\in T_j}u_{i,1} t(\balpha_i) +\sum_{i \in T_j, 2 \le r \le n}P_{i,r}t_r \\
	c &= \sum_{i\in T_j}u_{i,1} t(\balpha_i)^2  +2  \sum_{i \in T_j, 2 \le r \le n}t(\balpha_i) P_{i,r}t_r    
	+2 \sum_{i \in T_j, 2 \le r \le s \le n} P_{i,r,s}t_rt_s.
	\end{align*}
	Suppose first that $j$ is in $\mathfrak{a}$. Then, $T_j=\{\sigma_j\}$, so we 
	have only one term $\Lambda_{\sigma_j}$ to consider, and $Q_{\sigma_j}$ 
	is reduced, so that all coefficients $P_{\sigma_j,r}$ and
	$P_{\sigma_j,r,s}$ vanish. Thus, we are left in
	this case with
	$$
	a = u_{\sigma_j,1}, \quad
	b = u_{\sigma_j,1} t(\balpha_{\sigma_j}), \quad
	c = u_{\sigma_j,1} t(\balpha_{\sigma_j})^2,
	$$ so that we have $ac=b^2$, for {\em any} choice of $\lambda$ and
	$t$. Now, we suppose that $j$ is not in $\mathfrak{a}$, and we prove
	that for a generic choice of $\lambda$ and $t$, $ac-b^2$ is non-zero.
	The quantity $ac-b^2$ is a polynomial in the coefficients
	$(\bu_i)_{i\in T_j}$, and $(t_i)_{i \in \{2,\dots,n\}}$, and we have
	to show that it is not identically zero. We discuss two cases; in both
	of them, we prove that a suitable specialization of $ac-b^2$ is
	non-zero.
	
	Suppose first that for at least one index $\sigma$ in $T_j$,
	$Q_\sigma$ is not reduced. In this case, there exists as least one
	index $\rho$ in $\{2,\dots,n\}$ such that
	$P_{\sigma,\rho}(u_{\sigma,2},\dots,u_{\sigma,D_\sigma})$ is not
	identically zero \todo{explain better}. Let us set all $\bu_{\sigma'}$
	to $0$, for $\sigma'$ in $T_j-\{\sigma\}$, as well as $u_{\sigma,1}$,
	and all $t_r$ for $r\ne \rho$. Then, under this specialization,
	$ac-b^2$ becomes
	$-(P_{\sigma,\rho}(u_{\sigma,2},\dots,u_{\sigma,D_\sigma})t_\rho)^2$,
	which is non-zero, so that $ac-b^2$ itself must be non-zero.
	
	Else, since $j$ is not in $\mathfrak{a}$, we can assume that $T_j$
	has cardinality at least $2$, with $Q_\sigma$ reduced for all $\sigma$
	in $T_j$ (so that $P_{\sigma,r}$ and $P_{\sigma,r,s}$ vanish for 
	all such $\sigma$ and all $r,s$). Suppose that $\sigma$ and $\sigma'$ are two indices in
	$T_j$; we set all indices $u_{\sigma'',1}$ to zero, for $\sigma''$
	in $T_j-\{\sigma,\sigma'\}$. We are left with
	$$
	a=u_{\sigma,1}+u_{\sigma',1},\quad
	b=u_{\sigma,1}t(\balpha_{\sigma})+u_{\sigma',1}t(\balpha_{\sigma'}),\quad
	c=u_{\sigma,1}t(\balpha_{\sigma})^2+u_{\sigma',1}t(\balpha_{\sigma'})^2.
	$$
	Then, $ac-b^2$ is equal to $2u_{\sigma,1}u_{\sigma',1}(t(\balpha_{\sigma})-t(\balpha_{\sigma'}))^2$,
	which is non-zero, since $\balpha_\sigma \ne \balpha_{\sigma'}$.
\end{proof}

The previous lemmas allow us to write Algorithm
$\mathsf{ParametrizationX}_1$. After computing $M$, we determine its
factor $P=\prod_{j \in \{1,\dots,c\}, m_j=1} (T-r_j)$. We split this
polynomial further using the previous results in order to find
$\prod_{j \in \mathfrak{a}} (T-r_j)$, and we conclude using the same
kind of calculations as in $\mathsf{ParametrizationGeneric}$.

\begin{algorithm}[H]
	\caption{$\mathsf{ParametrizationX}_1(\ell,t)$}
	~\\
	{\bf Input:} \vspace{-0.5em}
	\begin{itemize}\setlength\itemsep{0em}
		\item a linear form $\ell$ over $Q$
		\item a linear form $t=t_2 X_2 + \cdots + t_n X_n$
	\end{itemize}
	{\bf Output:} polynomials $((P,V_1,\dots,V_n),X_1)$
	\begin{enumerate}\setlength\itemsep{0em}
		\item let $M$ be the minimal polynomial of the sequence $(\ell(X_1^s))_{s \ge 0}$ and let $\delta$ be its degree
		\item let $P = \prod_{r \text{~root of $M$ of multiplicity 1}}(T-r)$
		\item let $B=T^\delta M(1/T)$
		\item let $t$ be a random linear form in $X_2,\dots,X_n$
		\item \textbf{for} $i=0,1,2$ \textbf{do}
		\begin{enumerate}
			\item let $A_i = \big (\sum_{s < \delta} \ell(t^i X_1^s)T^s\big )B \mod T^{\delta}$
			\item let $\tilde A_i = T^{\delta -1}A_i(1/T)$
		\end{enumerate}
		\item\label{step:updateP} let $P = \gcd(P, \tilde A_0 \tilde A_2-\tilde A_1^2)$
		\item \textbf{for} $i=2,\dots,n$ \textbf{do}
		\begin{enumerate}
			\item let $A_{X_i} = \big (\sum_{s < \delta} \ell(X_2 X_1^s)T^s\big)B \mod T^{\delta}$ 
			\item let $\tilde A_{X_i} = T^{\delta -1}A_{X_i}(1/T)$
		\end{enumerate}
		\item \textbf{return} $((P,T,\tilde A_{X_2}/\tilde A_1 \bmod P, \dots,\tilde A_{X_n}/\tilde A_{1} \bmod P),X_1)$
	\end{enumerate}
	\label{algo:para}
\end{algorithm}


\begin{Lemma}
	Suppose that $\ell$ is a generic element of ${\rm
		hom}_{\Kbar}(Q,\Kbar)$ and that $t$ is a generic linear form. Then
	the output $((P,V_1,\dots,V_n),X_1)$ of
	$\mathsf{ParametrizationX}_1(\ell,t)$ is a zero-dimensional
	parametrization of $V_{\mathfrak{A}}$.
\end{Lemma}
\begin{proof}
	Lemma~\ref{lemma:minpoly} shows that for a generic choice of $\ell$,
	$M$ is the minimal polynomial of $X_1$, so that we indeed have
	$P=\prod_{j \in \{1,\dots,c\}, m_j=1} (T-r_j)$. Let then $r_j$ be
	one of these roots; by Lemma~\ref{lemma:valuelambda}, for $i=0,1,2$
	we have $\tilde A_i(r_j) = M'(r_j) (t^i \cdot \lambda_j)(1)$, where
	$\lambda_j =\sum_{i \in T_j} \ell_i$, and the $\ell_i$'s are the
	components of $\ell$. 
	
	As a result, the value of $\tilde A_0 \tilde A_2 - \tilde A_1^2$ at
	$r_j$ is (up to the non-zero factor $M'(r_j)^2$) equal to the
	quantity $ac-b^2$ defined in Lemma~\ref{lemma:acb2}, so for a
	generic choice of $\ell$ and $t$, it vanishes if and only if $j$ is
	in $\mathfrak{a}$. Thus, after Step~\ref{step:updateP}, 
	$P$ is equal to $\prod_{j \in \mathfrak{a}} (T-r_j)$.
	
	The last step is to compute the zero-dimensional parametrization of
	$V_{\mathfrak{A}}$. This is done using again
	Lemma~\ref{lemma:valuelambda}. Indeed, for $j$ in $\mathfrak{a}$, 
	$T_j$ is simply equal to $\{\sigma_j\}$, so that we have, for $i=2,\dots,n$,
	$$\tilde A_1(r_j)=M'(r_j) \lambda_j(1) \quad\text{and}\quad \tilde
	A_{X_i}(r_j) = M'(r_j) (X_i \cdot \lambda_j)(1) = M'(r_j) \lambda_j(X_i).$$ Now, since $j$
	is in $\mathfrak{a}$, $Q_{\sigma_j}$ is reduced, so that there
	exists a constant $\lambda_{j,1}$ such that for all $f$ in
	$\Kbar[X_1,\dots,X_n]$, $\lambda_j(f)$ takes the form $\lambda_{j,1}
	f(\balpha_{\sigma_j})$. This shows that, as claimed,
	$$\frac{\tilde A_{X_j}(r_j)}{\tilde A_1 (r_j)} = \frac
	{M'(r_j) \lambda_{j,1} \alpha_{j,i}}{M'(r_j) \lambda_{j,1}} = \alpha_{j,i}.$$
	For $i=1$, since we use $X_1$ as a separating variable for $V_{\mathfrak{A}}$, 
	we simply add the polynomial $T$ to our list.
\end{proof}


\subsection{Computing the canonical generator of a linearly recurrent matrix sequence}
\label{section:matrix_seq}
In Sparse-FGLM algorithm, generating the matrix sequence 
$L = (u^{tr} \mT_1^i)_{(0 \le i < 2D)}$ is the bottleneck of this
algorithm. The most efficient way to compute $L^{(i)}$ is
to compute $L^{(i-1)}\mT_1$; however, this requires the terms
of $L$ to be computed sequentially. Therefore, it is natural to
consider using blocking methods; that is, using
sequences of small matrices instead of scalar
sequences. Computing each
term of such sequences will take longer, but this is easily
parallelizable. Therefore, if blocking reduces the number of terms
needed, we can expect an overall speed up.

The idea of extending the Wiedemann algorithm to using
blocking methods is due to Coppersmith \cite{Coppersmith93}.
The formal analysis of Coppersmith's algorithm was done by
Kaltofen, Villard, and others \cite{KaVi04}\cite{Villard97}.
As with the Wiedemann algorithm, we are mainly interested in
computing the minimal polynomial of a matrix from a sequence.

We first present the notion of linear recurrence for sequences of matrices over
a field $\field$, which extends the well-known notion for sequences in
$\field^\NN$.
\begin{definition}[{\cite[Sec.\,3]{KalVil01}}]
	%% also \cite[Def.\,4.2]{Turner02}.. any earlier ref?
	\label{dfn:recurrence_relation}
	Let $\seq = (\seqelt{k})_{k\in\NN} \subset \seqeltSpace$ be a
	matrix sequence.  Then,
	\begin{itemize}
		\item a polynomial $p = \sum_{0\le k\le \degBd} p_k \var^k \in \polRing$ is
		said to be a \emph{scalar relation for $\seq$} if $\sum_{0\le k \le
			\degBd} p_{k} \seqelt{\delta + k} = 0$ holds for all $\delta \ge 0$;
		\item a polynomial vector $\rel = \sum_{0\le k\le \degBd} p_k \var^k \in
		\relSpace$ is said to be a \emph{(left, vector) relation for $\seq$} if
		$\sum_{0 \le k \le \degBd} p_{k} \seqelt{\delta + k} = 0$ holds for all
		$\delta \ge 0$;
		\item $\seq$ is said to be \emph{linearly recurrent} if there exists a
		nontrivial scalar relation for $\seq$.
	\end{itemize}
\end{definition}
For designing efficient algorithms it will be useful to rely on operations on
polynomials or truncated series, hence the following characterization of vector
relations.

\begin{lemma}
	\label{lem:linearly_recurrent}
	Consider a matrix sequence $\seq = (\seqelt{k})_{k\in\NN} \subset
	\seqeltSpace$ and its generating series $\seqpm = \sum_{k\ge 0} \seqelt{k} /
	\var^{k+1} \in \field\Poxi^{\rdim \times \cdim}$.  Then, $\rel \in \relSpace$
	is a vector relation for $\seq$ if and only if the entries of $\num = \rel
	\seqpm$ are in $\polRing$; furthermore, in this case, $\deg(\num) <
	\deg(\rel)$.
\end{lemma}
\begin{proof}
	Let $\rel = \sum_{0 \le k \le \degBd} p_k \var^k$. For $\delta \ge 0$, the
	coefficient of $\num$ of degree $-\delta-1<0$ is $\sum_{0\le k \le \degBd}
	p_k \seqelt{k+\delta}$. Hence the equivalence, by definition of a relation.
	The degree comparison is clear since $\seqpm$ has only terms of (strictly)
	negative degree.
\end{proof}

Concerning the algebraic structure of the set of vector relations, we have the
following basic result, which can be found for example in
\cite{Villard97,KalVil01,Turner02}.

\begin{lemma}
	\label{lem:module_rank}
	The sequence $\seq$ is linearly recurrent if and only if the set of left
	vector relations for $\seq$ is a $\polRing$-submodule of $\relSpace$ of rank
	$\rdim$.
\end{lemma}
\begin{proof}
	The set of vector relations for $\seq$ is a $\polRing$-submodule of
	$\relSpace$, and hence is free of rank at most $\rdim$
	\cite[Chap.\,12]{DumFoo04}.
	
	If $\seq$ is linearly recurrent, let $p \in \polRing$ be a nontrivial scalar
	relation for $\seq$. Then each vector $[0 \; \cdots \; 0 \; p \; 0 \; \cdots
	\; 0]$ with $p$ at index $1 \le i \le \rdim$ is a vector relation for $\seq$,
	hence $\seq$ has rank $\rdim$.  Conversely, if $\seq$ has rank $\rdim$, then
	it has a basis with $\rdim$ vectors, which form a matrix in $\relbasSpace$;
	the determinant of this matrix is a nontrivial scalar relation for $\seq$.
\end{proof}

Note however that a matrix sequence may admit nontrivial vector relations and
have no scalar relation (and therefore not be linearly recurrent with the
present definition); in this case the module of vector relations has rank less
than $\rdim$.

\begin{definition}
	\label{dfn:matrix_generator}
	Let $\seq \subset \seqeltSpace$ be linearly recurrent.  A \emph{(left) matrix
		generator} for $\seq$ is a matrix in $\relbasSpace$ whose rows form a basis
	of the module of left vector relations for $\seq$. This basis is said to be
	\begin{itemize}
		\item \emph{minimal} if the matrix is row reduced \cite{Wolovich74,Kailath80};
		%\item \emph{ordered weak Popov} if the matrix is in weak Popov form
		%  \cite{MulSto03} with pivots on the diagonal;
		\item \emph{canonical} if the matrix is in Popov form \cite{Popov72,Kailath80}.
	\end{itemize}
\end{definition}

Note that the canonical generator is also a minimal generator; furthermore, all
matrix generators $\relbas \in \relbasSpace$ for $\seq$ have the same
determinantal degree $\deg(\det(\relbas))$, which we denote by $\degDet$.  We
now show that minimal matrix generators are denominators in some irreducible
fraction description of the generating series of the sequence.  This is a
direct consequence of \cref{lem:linearly_recurrent,lem:module_rank} and of
basic properties of polynomial matrices.

\begin{corollary}
	A matrix sequence $\seq = (\seqelt{k})_{k\in\NN} \subset \seqeltSpace$ is
	linearly recurrent if and only if its generating series $\seqpm = \sum_{k\ge
		0} \seqelt{k} / \var^{k+1} \in \field\Poxi^{\rdim \times \cdim}$ can be
	written as a matrix fraction $\seqpm = \relbas^{-1} \nummat$ where $\relbas
	\in \relbasSpace$ is nonsingular and $\nummat \in
	\polMatSpace[\rdim][\cdim]$. In this case, we have $\rdeg{\nummat} <
	\rdeg{\relbas}$ and $\deg(\det(\relbas)) \ge \degDet$, and $\relbas$ is a
	matrix generator of $\seq$ if and only if $\deg(\det(\relbas)) = \degDet$ or,
	equivalently, the fraction $\relbas^{-1} \nummat$ is irreducible (that is,
	$\mat{U} \relbas + \mat{V} \nummat = \mat{I}$ for some polynomial matrices
	$\mat{U}$ and $\mat{V}$).
\end{corollary}

We remark that we may also consider vector relations operating on the right: in
particular, \cref{lem:linearly_recurrent} shows that if the sequence is
linearly recurrent then these right relations form a submodule of
$\polMatSpace[\cdim][1]$ of rank $\cdim$. Thus, a linearly recurrent sequence
also admits a right canonical generator.

Now, we focus on our algorithmic problem: given a linearly recurrent sequence,
find a minimal matrix generator.  We assume the availability of bounds
$(\degBdl,\degBdr)$ on the degrees of the left and right canonical generators,
which allow us to control the number of terms of the sequence we will access
during the algorithm.  Since taking the Popov form of a reduced matrix does not
change the degree, any left minimal matrix generator $\relbas$ has the same
degree $\deg(\relbas)$ as the left canonical generator: thus, $\degBdl$ is also
a bound on the degree of any left minimal generator. The same remark holds for
$\degBdr$ and right minimal generators.  (These bounds $\degBdl,\degBdr$ are
the same as $\gamma_1,\gamma_2$ in \cite[Def.\,4.6~and\,4.7]{Turner02}; see
also $\delta_l,\delta_r$ in \cite[Sec.\,4.2]{Villard97a}.)

\begin{lemma}
	\label{lem:finitely_many_terms}
	Let $\seq = (\seqelt{k})_k \subset \seqeltSpace$ be linearly recurrent and
	let $\degBdr \in \NN$ be such that the right canonical matrix generator of
	$\seq$ has degree at most $\degBdr$.  Then, $\rel = \sum_{0\le k\le \degBd}
	p_k \var^k \in \relSpace$ is a left relation for $\seq$ if and only if
	$\sum_{0 \le k \le \degBd} p_{k} \seqelt{\delta + k} = 0$ holds for $\delta
	\in \{0,\ldots,\degBdr-1\}$.
\end{lemma}
\begin{proof}
	Since the right canonical generator $\relbas \in \polMatSpace[\cdim]$ is in
	column Popov form, we have $\relbas =
	\mat{L}\diag{\var^{t_1},\ldots,\var^{t_\cdim}} - \mat{Q}$ where
	$\cdeg{\mat{Q}} < \cdeg{\relbas} = (t_1,\ldots,t_\cdim)$ componentwise and
	$\mat{L} \in \matSpace[\cdim]$ is unit upper triangular. We define the matrix
	$\mat{U} = \diag{\var^{\degBdr-t_1},\ldots,\var^{\degBdr-t_\cdim}}
	\mat{L}^{-1}$, which is in $\polMatSpace[\cdim]$ since $\degBdr \ge
	\deg(\relbas) = \max_j t_j$. Then, the columns of the right multiple $\relbas
	\mat{U} = \var^{\degBdr} \mat{I}_\cdim - \mat{Q} \mat{U}$ are right relations
	for $\seq$, and we have $\deg(\mat{Q} \mat{U}) < \degBdr$. As a consequence,
	writing $\mat{Q} \mat{U} = \sum_{0 \le k < \degBdr} Q_k \var^k$, we have
	$\seqelt{\degBdr+\delta} = \sum_{0 \le k < \degBdr} \seqelt{k+\delta} Q_k$
	for all $\delta \ge 0$.
	
	Assuming that $\sum_{0 \le k \le \degBd} p_{k} \seqelt{\delta + k} = 0$ holds
	for all $\delta \in \{0,\ldots,\degBdr-1\}$, we prove by induction that this
	holds for all $\delta\in\NN$. Let $\delta \ge \degBdr-1$ and assume that this
	identity holds for all integers up to $\delta$. Then, the identity concluding
	the previous paragraph implies that
	\begin{align*}
	\sum_{0 \le k \le \degBd} p_{k} \seqelt{\delta+1 + k} & =
	\sum_{0 \le k \le \degBd} p_{k} \left(\sum_{0\le j<\degBdr} \seqelt{\delta+1+k-\degBdr+j} Q_j\right) \\
	& = \sum_{0\le j<\degBdr} 
	\underbrace{\left(\sum_{0 \le k \le \degBd} p_{k} \seqelt{\delta+1-\degBdr+j+k}\right)}_{=\, 0 \text{ since } \delta+1-\degBdr+j \le \delta} Q_j = 0,
	\end{align*}
	and the proof is complete.
\end{proof}

(A similar result is in \cite[Thm.\,4.5]{Turner02}.)

The fast computation of matrix generators is usually handled via algorithms for
computing minimal approximant bases \cite{Villard97,Turner02,GioLeb14}. The
next result gives the main idea behind this approach. This result is similar to
\cite[Thm.\,4.6]{Turner02} (see also
\cite[Thm.\,4.7,\,4.8,\,4.9,\,4.10]{Turner02}), but in some sense the reversal
is on the input sequence rather than on the output matrix generator.

We recall from \cite{BarBul92,BecLab94} that, given a matrix $\sys \in
\polMatSpace[\rdim][\cdim]$ and an integer $d \in \NN$, the set of
\emph{approximants for $\sys$ at order $d$} is defined as
\[
\appMod{\sys}{d} = \{ \rel \in \relSpace \mid \rel \sys = 0 \bmod \var^d \}.
\]

Then, the next theorem shows that relations for $\seq$ can be retrieved as
subvectors of approximants at order about $\degBdl+\degBdr$ for a matrix
involving the first $\degBdl+\degBdr$ entries of $\seq$. 

\begin{theorem}
	\label{thm:mingen_via_appbas}
	Let $\seq = (\seqelt{k})_k \subset \seqeltSpace$ be a linearly recurrent
	sequence and let $(\degBdl,\degBdr) \in \NN^2$ be such that the left
	(resp.~right) canonical matrix generator of $\seq$ has degree
	$\le\degBdl$ (resp.~$\le \degBdr$).
	
	For $\degBd>0$, define
	\begin{equation}
	\label{eqn:series_to_approximate}
	\sys =
	\begin{bmatrix}
	\sum_{0\le k < \degBd} \seqelt{k} \var^{\degBd-k-1} \\ - \mat{I}_{\cdim}
	\end{bmatrix} \in \polMatSpace[(\rdim+\cdim)][\cdim].
	\end{equation}
	For any relation $\rel \in \relSpace$ for $\seq$, there exists $\rem \in
	\remSpace$ such that $\deg(\rem) < \deg(\rel)$ and $[\rel \;\; \rem] \in
	\appMod{\sys}{\degBd}$.  Assuming $\degBd \ge \degBdr+1$, for any vectors
	$\rel \in \relSpace$ and $\rem \in \remSpace$, if $[\rel \;\; \rem]
	\in\appMod{\sys}{\degBd}$ and $\deg([\rel \;\; \rem])\le\degBd-\degBdr-1$,
	then $\rel$ is a relation for $\seq$. %% and $\deg(\rem)<\deg(\rel)$.
	
	As a corollary, if $\mat{B} \in \polMatSpace[(\rdim+\cdim)][(\rdim+\cdim)]$
	is a basis of $\appMod{\sys}{\degBdl+\degBdr+1}$, then
	\begin{itemize}
		\item if $\mat{B}$ is in Popov form then its $\rdim\times\rdim$ leading
		principal submatrix is the canonical matrix generator for $\seq$;
		\item if $\mat{B}$ is row reduced then it has exactly $\rdim$ rows of
		degree $\le\degBdl$, and the corresponding submatrix $[\relbas \;\;
		\remmat]$ of $\mat{B}$ is such that $\relbas\in\relSpace$ is a minimal
		matrix generator for $\seq$.
		%the form $[\rel \;\; \rem]$ with $\rel \in \relSpace$, $\rem \in
		%\remSpace$, and $\deg(\rem) < \deg(\rel) \le \degBdl$
	\end{itemize}
\end{theorem}
\begin{proof}
	From \cref{lem:linearly_recurrent}, if $\rel$ is a relation for $\seq$ then
	$\num = \rel \seqpm$ has polynomial entries, where $\seqpm = \sum_{k\ge 0}
	\seqelt{k} \var^{-k-1}$. Then, the vector $\rem = - \rel (\sum_{k \ge \degBd}
	\seqelt{k} \var^{\degBd-k-1})$ has polynomial entries, has degree less than
	$\deg(\rel)$, and is such that $[\rel \;\; \rem] \sys = \num \var^{\degBd}$,
	hence $[\rel \;\; \rem] \in \appMod{\sys}{\degBd}$.
	
	Conversely, if $[\rel \;\; \rem] \in\appMod{\sys}{\degBd}$ we have $\rel
	(\sum_{0\le k < \degBd} \seqelt{k} \var^{\degBd-k-1}) = \rem \bmod
	\var^\degBd$. Since $\degBd\ge\degBdr+1$ and $\deg([\rel \;\;
	\rem])\le\degBd-\degBdr-1$, this implies that the coefficients of degree
	$\degBd-\degBdr$ to $\degBd-1$ of $\rel(\sum_{0\le k < \degBd} \seqelt{k}
	\var^{\degBd-k-1})$ are zero. Then, \cref{lem:finitely_many_terms} shows that
	$\rel$ is a relation for $\seq$.
	
	Finally, the two items are straightforward consequences.
\end{proof}

Then, using fast approximant basis algorithms, we obtain the next result.

\begin{corollary}
	Let $\seq \subset \seqeltSpace$ be a linearly recurrent sequence and let
	$\degBd = \degBdl+\degBdr+1$, where $(\degBdl,\degBdr) \in \NN^2$ are such
	that the left (resp.~right) canonical matrix generator of $\seq$ has degree
	$\le\degBdl$ (resp.~$\le \degBdr$).  Then,
	\begin{itemize}
		\item using the algorithm of \cite{GiJeVi03}: if $\cdim \in \Omega(\rdim)$,
		a left minimal matrix generator for $\seq$ can be computed in $O(\cdim^\omega
		\mathsf{M}(\degBd) \log(\degBd))$ operations in $\field$;
		\item using the algorithm of \cite{ZhoLab12}: if $\cdim \in O(\rdim)$, a
		left minimal matrix generator for $\seq$ can be computed in $O(\rdim^\omega
		\mathsf{M}(\cdim\degBd/\rdim) \log(\cdim\degBd))$ operations in $\field$;
		\item using the algorithm of \cite{JeNeScVi16}: the left canonical matrix
		generator for $\seq$ can be computed in $O((\rdim+\cdim)^{\omega-1}
		\mathsf{M}(\cdim\degBd) \log(\cdim\degBd)^3)$ operations in $\field$.
	\end{itemize}
\end{corollary}

In our algorithm, we will always use $\cdim = \rdim$; therefore we can find the
left canonical matrix generator in $O(\rdim^\omega \mathsf{M}(\degBd) \log(\degBd))$
operations in $\field$.


\newpage
\section{Block Sparse-FGLM algorithm}
In this section, we will show how to extend the Sparse-FGLM
to using blocking methods. Steel's method also uses the
Block Wiedemann algorithm to compute the minimal polynomial
of $\mM_i$, for which the roots provide the appropriate
values for $X_i$,
but uses the ``evaluation" method for the rest
(another Gr\"obner Basis computation with that variable
evaluated at each root of the minimal polynomial) \cite{Steel15}.
Our algorithm computes the rest of the lex
Gr\"obner basis directly.

Given:
\begin{center}
	\begin{tabular}{c c}
		$I \subset \mathbb{K}$:& zero dimensional ideal
		in shape position\\
		$\mathbb{B} \subset \mathbb{K}[X_1,\dots,X_n]/I$:&
		monomial basis of $\mathbb{K}[X_1,\dots,X_n]/I$\\
		$D$: & dimension of $\mathbb{B}$\\
		$\mM_1, \dots,\mM_n$:& multiplication matrices of
		$X_1 ,\dots,X_n$ respectively\\
		$t$:& random linear combination of $X_i$'s\\
		$\mM$:& multiplication matrix of $t$
	\end{tabular}
\end{center}
we compute a lex Gr\"obner basis that have the same points
in its variety as the radical of $I$ 
(note that we do not assume that $I$ is radical).
This is because we introduce another variable $t$ which,
generically, separates the points in the variety.
We also assume that the base field $\K$ has characteristic
larger than $D$. More precisely, we want to find polynomials
$(R,R_1,\dots,R_n)$ such that for all $\alpha$ that is a factor of
$R$, $\{ (R_1(\alpha),\dots,R_n(\alpha)) \} = V(I)$

The main idea is to compute the minimal polynomial
of $\mM$ by using the block Wiedemann algorithm.
This is done by applying the Matrix Berlekamp-Massey algorithm of
section \ref{section:matrix_seq} on sequence 
$S = (\mU^{tr}\mM^i\mV)_{0 \le i < 2 \lceil \frac{D}{m} \rceil}$,
for some $m \in \mathbb{N}$ and $U,V \in \mathbb{K}^{m \times D}$,
and taking the largest invariant factor.
We find the rest of the polynomials in the output by extracting
the scalar closed forms from the matrix closed forms.


\begin{algorithm}[H]
	\label{algo:block-sparse-fglm}
	\caption{Block Sparse-FGLM($\mM,\mM_1,\dots,\mM_n,m$)}
	{\bf Input:} \vspace{-0.5em}
	\begin{itemize}
		\item $\mM,\mM_1,\dots,\mM_n$ defined as above
		\item dimension of the blocks $m \in \mathbb{N}$
	\end{itemize}
	{\bf Output:} polynomials $(R,R_1,\dots,R_n)$
	\begin{itemize}\setlength\itemsep{0em}
		\item[]{\bf 1.~} {\sf choose $\mU,\mV \in \mathbb{K}^{m \times D}$}
		\item[]{\bf 2.~} {\sf $S= (\mU^{tr}\mM^i\mV)_{0 \le i < 2d}$, with $d = \frac{D}{m}$}
		\item[]{\bf 3.~} {\sf $\mF^{U,M,V} = {\sf MatrixBerlekampMassey}(S)$}
		\item[]{\bf 4.~} {\sf $\mN^* = \mF^{U,M,V}\sum_{i=0}^{d-1} {(\mU^{tr}\mM^ie)}/{T^{i+1}}$} 
		\item[]{\bf 5.~} {\sf filter terms of $\mN^*$ with negative exponents} 	 
		\item[]{\bf 6.~} {\sf $P=$ largest invariant factor of $\mF^{U,M,V}$}
		\item[]{\bf 7.~} {\sf $R={\sf SquareFreePart}(P)$} 
		\item[]{\bf 8.~} {\sf $a = [0 ~\cdots 0 P] (\mF^{U,M,V})^{-1}$}
		\item[]{\bf 9.~} {\sf $N=a\mN^*$}
		\item[]{\bf 10.~} {\sf for $j = 1 \dots n$:}
		\begin{itemize}
			\item[]{\bf 10.1.} ~~{\sf	$\mN_j^* = \mF^{U,M,V}
				\sum_{i= 0}^{d-1} {(\mU^{tr}\mM^i \mM_j e)}/{T^{i+1}}$}
			\item[]{\bf 10.2.} ~~{\sf $N_j=a\mN_j^*$}
			\item[]{\bf 10.3.} ~~{\sf $R_j=N_j/N$ mod $R$}
		\end{itemize}
	\end{itemize}
\end{algorithm}

\subsection{Proof of Correctness}

First, we present a proof that one can compute
the minimal polynomial of $\mM$ from the
output of Matrix Berlekamp-Massey if we choose
the blocking matrices $\mU$ and $\mV$ generically.
This result had been previously been proven by
Kaltofen and Villard \cite{KaVi04}\cite{Villard97a}.

Let $\mM$ be in $\mathbb{K}^{D \times D}$ and 
$\mU,\mV \in \mathbb{K}^{D \times m}$ be two blocking matrices. Now, define two matrix sequences
$S_V = (\mM^i \mV)_{(i \ge 0)}$ and 
$S_{U,V} = (\mU^{tr} \mM^i \mV)_{(i \ge 0)}$ denote their minimum
generating matrix polynomial as $\mF^{M,V}$ and $\mF^{U,M,V}$ respectively.
Let $s_1, \dots, s_r$ be the invariant factors
of $T\mI - \mM$, ordered in such a way that 
$s_r | s_{r-1}| \dots | s_1$, and let $d_i = $ deg$(s_i)$ for
all $i$; for $i > r$, we let $s_i$ = 1, with $d_i = 0$.
We define $\nu = d_1 + \cdots + d_m \le D$ and
$\delta = \lceil \nu / m \rceil \le \lceil D / m \rceil$.
We also denote by $\sigma_1, \cdots, \sigma_k$ the invariant
factors of $\mF^{U,M,V}$, for some $k \le m$. As above,
for $i > k$, we let $\sigma_i = 1$.

\begin{theorem}
	\label{randXY}
	For a generic choice of $\mU$ and $\mV$, we have:
	\begin{itemize}
		\item $\mF^{U,M,V}$ has degree $\delta$;
		\item $s_i = \sigma_i$ for $1 \le i \le m$.
	\end{itemize}
\end{theorem}

\begin{proof}
	We denote by $\langle \mV \rangle$ the vector
	space generated by the columns of $\mV,\mM\mV,\mM^2\mV,\dots$. We also write
	$D_V=\dim(\langle \mV \rangle)$.
	
	First, we prove that for any $\mV$ in $\K^{D \times m}$, for a generic
	$\mU$ in $\K^{D\times m}$, $\mF^{U,M,V}=\mF^{M,V}$.  Indeed,
	by~\cite[Lemma~4.2]{Villard97a}, there exists matrices $\mathbf{P}_V$ in
	$\K^{D\times D_V}$ and $\mM_V \in \K^{D_V \times D_V}$, with $\mathbf{P}_V$ of
	full rank $D_V$, and where $\mM_V$ is a matrix of the restriction of $\mM$
	to $\langle \mV \rangle$, such that $\mF^{U,M,V}=\mF^{M,V}$ if and only if
	the dimension of the span of $[\mZ ~ \mB_V \mZ ~\mB_V^2 \mZ ~ \cdots]$ is equal to
	$D_V$, with $\mB_V=\mM_V^\perp$ and $\mZ=\mathbf{P}_V^\perp \mU \in \K^{D_V \times m}$.
	
	We prove that this is the case for a generic $\mU$. By construction, one
	can find a basis of $\langle \mV \rangle$ in which the matrix of $\mM_V$
	is block-companion, with $m' \le m$ blocks (take the $\mM_V$-span of the
	first column of $\mV$, then of the second column, working modulo the
	previous vector space, etc.) Thus, $\mB_V$ is similar to a
	block-companion matrix with $m'$ blocks as well; since $\mZ$ has $m$
	columns, $S$ has full dimension $N_V$ for a generic $\mZ$ (and for a
	generic $\mU$, since $P_V$ has rank $N_V$). Thus, for generic choices of
	$\mU$ and $\mV$, $\mF^{U,M,V}=\mF^{M,V}$.
	
	Let us next introduce a matrix $\scrV$ of indeterminates of size $D
	\times m$, and let $\mF^{M,\scrV}$ be the minimal generating polynomial
	of the ``generic'' sequence $(\mM^i \scrV)_{i \ge 0}$. The notation
	$\langle \scrV \rangle$ and $D_\scrV$ are defined as above.  In
	particular, by~\cite[Proposition 6.1]{Villard97a}, the minimal
	generating polynomial $\mF^{M,\scrV}$ has degree $\delta$ and
	determinantal degree $\nu$.
	
	Now, for a generic $\mV$ in $\K^{D\times m}$, $D_V=D_\scrV$. Indeed,
	$\langle \scrV\rangle$ is the span of $K_\scrV=[\scrV ~ \mM \scrV ~ \cdots ~
	\mM^{N-1} \scrV]$, whereas $\langle \mV \rangle$ is the span of $[\mV ~ \mM \mV
	~ \cdots ~\mM^{N-1} \mV]$. Take a maximal non-zero minor $\mu$ of
	$K_\scrV$; as soon as $\mu(Y)\ne 0$, we have equality of the
	dimensions. On the other hand, by~\cite[Lemma~4.3]{Villard97a}, for
	any $\mV$ (including $\scrV$), the degree of $\mF^{M,V}$ is equal to the
	first index $d$ such that $\dim({\rm span}([\mV ~ \mM \mV ~ \cdots ~\mM^{d-1}
	\mV]))=D_V$. As a result, for generic $\mV$, $\mF^{M,V}$ and $\mF^{M,\scrV}$
	have the same degree, that is, $\delta$.  The first item is proved.
	
	We conclude by proving that for generic $\mU,\mV$, the invariant factors
	$\sigma_1,\dots,\sigma_m$ of $\mF^{U,M,V}$ are $s_1,\dots,s_m$.
	By~\cite[Theorem~2.12]{KaVi04}, for any $\mU$ and $\mV$ in $\K^{D\times
		m}$, for $i=1,\dots,m$, the $i^{th}$ invariant factor $\sigma_i$ of
	$\mF^{U,M,V}$ divides $s_i$, so that $\deg(\det(\mF^{U,M,V}))\le\nu$, with
	equality if and only if $\sigma_i=s_i$ for all $i \le m$.
	
	For $\mV$ as above and any integers $e,d$, we let ${\rm Hk}_{e,d}(\mV)$ be
	the block Hankel matrix
	$$ {\rm Hk}_{e,d}(\mV) =
	\begin{bmatrix}
	\mI \\  \mM \\  \mM^2 \\ \vdots  \\  \mM^{e-1}
	\end{bmatrix}
	\begin{bmatrix}
	\mV & \mM\mV & \mM^2\mV &\cdots&  \mM^{d-1}\mV
	\end{bmatrix}
	$$ By~\cite[Eq.~(2.6)]{KaVi04}, ${\rm rank}({\rm Hk}_{e,d}(\mV)) =
	\deg(\det(\mF^{M,V}))$ for $d \ge \deg(\mF^{M,V})$ and $e \ge D$.  We take
	$e=D$, so that ${\rm rank}({\rm Hk}_{D,d}(\mV)) = \deg(\det(\mF^{M,V}))$
	for $d \ge \deg(\mF^{M,V})$. On the other hand, the sequence ${\rm
		rank}({\rm Hk}_{N,d}(\mV))$ is constant for $d \ge D$; as a result,
	${\rm rank}({\rm Hk}_{D,D}(\mV)) = \deg(\det(\mF^{T,V}))$. For the same
	reason, we also have ${\rm rank}({\rm Hk}_{D,D}(\scrV)) =
	\deg(\det(F^{M,\scrV}))$, so that for a generic $\mU$, $\mF^{M,V}$ and
	$\mF^{M,\scrV}$ have the same determinantal degree, that is, $\nu$.  As
	a result, for generic $\mU$ and $\mV$, we also have
	$\deg(\det(\mF^{U,M,V}))=\nu$, and the conclusion follows.
\end{proof} 

It is well known that the largest invariant factor of
$T\mI - \mM$ is the minimal polynomial of $\mM$. Therefore, the above
theorem shows that if we choose the entries of $\mU$ and $\mV$
randomly, we will have that the
largest invariant factor of $\mF^{U,M,V}$ is the minimal
polynomial of $\mM	$  with high probability.

Next, we prove that one can recover a scalar numerator
through a matrix numerator.

\begin{Lemma}\label{utilde}
	Let $\textbf{a}$ be defined as line 8 of algorithm \ref{algo:block-sparse-fglm}, 
	then $\textbf{a}$ has polynomial entries.
\end{Lemma}

\begin{proof}
	Let $\mathscr{D} = \mA\mF^{U,M,V}\mB$ be the Smith normal form of $\mF^{U,M,V}$ and $s_1, \cdots s_m$ be
	invariant factors of $\mF^{U,M,V}$ such that 
	$s_m | s_{m-1} | \cdots | s_1$.
	Let $[b_1,\cdots,b_m]$ be the last row of $\mB$ and 
	$w = [\frac{s_1b_1}{s_m},\frac{s_1b_2}{s_{m-1}},\cdots,\frac{s_1b_{m-1}}{s_2},b_m]$ (since $s_i | s_1$), then
	\begin{align*}
	(w \mA) \mA^{-1} \mathscr{D} &=  [\frac{s_1b_1}{s_m},\frac{s_1b_2}{s_{m-1}},\cdots,\frac{s_1b_{m-1}}{s_2},b_m]
	\begin{bmatrix}
	s_m &        & \\
	& \ddots & \\
	&        & s_1
	\end{bmatrix}\\
	&= [s_1b_1, s_1b_2, \cdots, s_1b_m]\\
	&= [0,\cdots,0,s_1] \mB
	\end{align*}
	Therefore, if $a = w \mA$, we get
	$ a \mF^{U,M,V} = (w \mA) \mA^{-1} \mathscr{D} \mB^{-1} = 
	[0,\cdots,0,s_1]$ as needed. Since both $w$ and $\mA$ have polynomial
	entries, $a$ must also have polynomial entries.\\
\end{proof}

\begin{theorem}
	 Let $U = [u_1,u_2,\dots,u_m]$be in $\K^{D\times m}$,
	 $\mF^{U,M,V}$ be the minimum generator of $(\mU^t \mM^i \mV)_{i\ge0}$,
	 and $P$ be the minimal polynomial of $\mM$. For any 
	 $v \in \K^{D}$, if\\ 
	 $\mN^* = \mF^{U,M,V}\sum_{i=0}^{d-1} (\mU^t \mM^i v)/T^{i+1}$ 
	 with all terms of negative exponents removed
	 and $N = a\mN^*$,
	 then $N = \Omega((u_m^t \mM^i v)_{i\ge0},P)$.
	 \label{theorem:anyv}
\end{theorem}

\begin{proof}
	Let $Z = \sum_{i=0}^{\infty} (\mU^t\mM^iv)/T^{i+1}$ and
	$\mN^{*'} = \mF^{U,M,V} Z$.
	Since the highest power of the entries in 
	$Z$ is $T^{-1}$, the 
	entries of the product $\mN^{*'}$ must have degree less 
	than $d = deg(\mF^{U,M,V})$. Furthermore, since $\mF^{U,M,V}$ cancels 
	the sequence $(\mU\mM^i\mV)_{i \ge 0}$, $\mN^{*'}$ is a
	polynomial matrix and does not have terms of negative exponents.
	This means any terms in $Z$ with degree less than $-d$ must vanish in the
	product.
	Therefore,
	$$ \mN^* = \mN^{*'}$$
	
	Now, rewrite $\mU$ as $\mU = [u_1 u_2 \cdots u_m]$, then
	$$
	\mN^* = \mF^{U,M,V}
	\sum_{i=0}^{\infty} \mU^{tr} \mM^i v / T^{i+1} =
	\begin{bmatrix}
	\sum_{i\ge 0} u_1^{tr}\mM^iv/T^{i+1}\\
	\vdots                   \\
	\sum_{i\ge 0} u_m^{tr}\mM^iv/T^{i+1}
	\end{bmatrix}$$
	Recall $\Omega$ from definition \ref{def:omega}.
	By rewriting each $\sum_{i\ge0} u_j \mM^i v$ in its closed form, we get
	$$ \mN^* = \mF^{U,M,V}
	\begin{bmatrix}
	\Omega( (u_1^{tr} \mM^i v)_{i\ge 0},P) / P \\
	\vdots      \\
	\Omega((u_m^{tr} \mM^i v)_{i\ge 0},P) / P 
	\end{bmatrix}
	$$
	By theorem \ref{randXY}, the $i^{th}$ invariant factor of
	$T\mI - \mM$ is equal to the $i^{th}$ invariant factor of $\mF^{U,M,V}$ 
	for generic choice of
	$\mU,\mV$. Thus, $s_1 = P$ and by lemma \ref{utilde}
	\begin{align*}
	a \mN^* &= a \mF^{U,M,V}
	\begin{bmatrix}
	\Omega((u_1^{tr} \mM^i v)_{i\ge 0},P) / P \\
	\vdots      \\
	\Omega((u_m^{tr} \mM^i v)_{i\ge 0},P) / P 
	\end{bmatrix}\\
	&= [0,\cdots,0,P]
	\begin{bmatrix}
	\Omega((u_1^{tr} \mM^i v)_{i \ge 0},P) / P \\
	\vdots      \\
	\Omega((u_m^{tr} \mM^i v)_{i\ge 0},P) / P 
	\end{bmatrix}\\
	&= \Omega((u_m^{tr} \mM^i v)_{i\ge 0},P)
	\end{align*}
	Therefore, $N = \Omega(( u_m^{tr} \mM^i v)_{i\ge 0},P)$ as needed.
\end{proof}

% l_{u_m} for linear form of u_m
Finally, we conclude by seeing what happens
when we pick specific values for $v$ and apply theorem \ref{theorem:anyv}.
In line 3 of algorithm \ref{algo:block-sparse-fglm}, we compute
$\mN^* = \mF^{U,M,V}\sum_{i=0}^{d-1}\mU^t \mM e/T^{i+1}$, 
where $e$ is the coordinate vector of $1$ in $\mathbb{B}$. By applying theorem \ref{theorem:anyv},
$$N = a\mN^* = \Omega((u_m^{tr} \mM^i e)_{i\ge 0},P)$$
By construction,
$\mM^i e$ gives the coordinate vector of $t^i$ in $\mathbb{B}$, so
$$\sum_{i= 0}^{d-1} u_m^{tr} \mM^i e/T^{i+1} = \sum_{i\ge0} \ell_{u_m}(t^i)/T^{i+1}$$ 
where $\ell_{u_m}$ is the linear form associated with $u_m$. Therefore, 
$$N = \Omega((u_m^{tr} \mM^i e)_{i\ge 0},P) = \Omega( (\ell_{u_m}(t^i))_{i\ge 0},P)$$.

Similarly, in line 7.1 of algorithm \ref{algo:block-sparse-fglm}, we compute
$\mN^*_j = \mF^{U,M,V}\sum_{i\ge0} \mU^{tr} \mM \mM_j e / T^{i+1}$. By theorem \ref{theorem:anyv},
$$N_j = a \mN^*_j = \Omega((u_m^{tr} \mM^i \mM_j e)_{i\ge 0},P) $$
Again, by construction, $\mM_j e$ is the coordinate vector for $X_j$ in 
$\mathbb{B}$ and $\mM^i \mM_j e$ is the coordinate vector for $X_jt^i$ in
$\mathbb{B}$. Therefore,
$$ N_j =  \Omega((u_m^{tr} \mM^i \mT_j e)_{i\ge 0},P) =
 \Omega( (\ell_{u_m}(X_jt^i))_{i\ge 0},P)$$
By the correctness of algorithm \ref{algo:para2}, we have the proof
of correctness for algorithm \ref{algo:block-sparse-fglm}.

\newpage
\subsection{Example}
First, we give an example in the radical case. Let
$$I = \langle -16X_1^2 - 15X_1X_2 - 14X_2^2 - 48X_1 + 26, 35X_1X_2 + 47X_1 - 46X_2 - 47 \rangle \subset GF(101)[X_1,X_2]$$
We choose $t = 2X_1 + 53 X_2$, with multiplication matrices:
\begin{align*}
\mM_1 = \begin{bmatrix}
 85&   0&  37&   0\\
 69&  85&  15&   0\\
100&  91&  19&   1\\
  1&  10&  68&   0
\end{bmatrix}\,
\mM_2 = \begin{bmatrix}
36&  1&  0&  0\\
42&  0& 85&  1\\
51&  0& 91&  0\\
95&  0& 10&  0
\end{bmatrix}\,
\mM = \begin{bmatrix}
58& 53& 74&  0\\
41& 69& 91& 53\\
75& 81& 13&  2\\
88& 20& 60&  0
\end{bmatrix}
\end{align*}
We choose $m = 2$ and choose $U,V \in GF(101)^{D\times m}$ of random
entries:
$$ \mU^{tr} = \begin{bmatrix}
84& 38\\
29& 58\\
80& 43\\
 7& 82
\end{bmatrix}\,
\mV = \begin{bmatrix}
  6&  97\\
 83&  58\\
101&  95\\
 59&  89
\end{bmatrix}
$$
We compute the matrix sequence $S = (\mU^{tr}\mM^i\mV)_{i\ge0}$ and its minimum generating matrix polynomial $\mG$
$$ S = (
\begin{bmatrix}
92& 75\\  
83& 51
\end{bmatrix},
\begin{bmatrix}
57& 82\\  
23& 16
\end{bmatrix},
\begin{bmatrix}
54& 93\\  
70& 66
\end{bmatrix},
\begin{bmatrix}
50& 77\\
26& 76
\end{bmatrix}
)$$
$$ \mF^{U,M,V} =
\begin{bmatrix}
T^2 + 76T + 8&       87T + 31\\
    100T + 46& T^2 + 87T + 44
\end{bmatrix}
$$
The biggest invariant factor of $\mF^{U,M,V}$ is 
$$P = T^4 + 62T^3 + 85T^2 + 69T + 37 = R$$
since $P$ is square free. 
Now, we compute $\mN^*$ and $a$:
\begin{align*}
\mN^* &=[84T + 46, 38T + 65]\\
a &= [T + 55, T^2 + 76T + 8]
\end{align*}
Finally, we find the scalar numerator $N = a\mN^*$:
$$ N = 100T^3 + 26T^2 + 33T + 18$$
To get $R_1(T)$, we compute $\mN_1^*$ and $N_1=a\mN_1^*$:
\begin{align*}
\mN_1^* &= [79T + 8, 100T + 23]\\
N_1 &= 100T^3 + 26T^2 + 33T + 18
\end{align*}
Lastly,
\begin{align*}
R_1(T) &= N_1 / N \mod R \\
       &= 61T^3 + 75T^2 + 85T + 23
\end{align*}
We compute $R_2(t)= 32T^3 + 41T^2 + 94T + 22$ in the same way. 
As a sanity check,
we see that $V(I)$ has one point in $GF(101)^2$:
$(54,79)$ and $P$ has one factor in $GF(101)$: $53$. Now,
$R_1(53) = 54$ and $R_2(53) = 79$ as expected.
\newpage
\section{Experimental Results}

\newpage
\bibliographystyle{plain}
\bibliography{thesis}
\end{document}